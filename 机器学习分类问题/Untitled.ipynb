{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 7)\n",
      "(529, 6)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['train'] = 1\n",
    "# test['train'] = 0\n",
    "# data = pd.concat([train, test], axis=0, ignore_index=True)\n",
    "# print(data.shape)\n",
    "# display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['clear', 'green', 'black', 'white', 'blue', 'blood'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.color.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ghoul', 'Goblin', 'Ghost'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dict = {'Ghoul':2, 'Goblin':0, 'Ghost':1}\n",
    "train.loc[:, 'type'] = train['type'].map(type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bone_length</th>\n",
       "      <th>rotting_flesh</th>\n",
       "      <th>hair_length</th>\n",
       "      <th>has_soul</th>\n",
       "      <th>color</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.354512</td>\n",
       "      <td>0.350839</td>\n",
       "      <td>0.465761</td>\n",
       "      <td>0.781142</td>\n",
       "      <td>clear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.575560</td>\n",
       "      <td>0.425868</td>\n",
       "      <td>0.531401</td>\n",
       "      <td>0.439899</td>\n",
       "      <td>green</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.467875</td>\n",
       "      <td>0.354330</td>\n",
       "      <td>0.811616</td>\n",
       "      <td>0.791225</td>\n",
       "      <td>black</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.776652</td>\n",
       "      <td>0.508723</td>\n",
       "      <td>0.636766</td>\n",
       "      <td>0.884464</td>\n",
       "      <td>black</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.566117</td>\n",
       "      <td>0.875862</td>\n",
       "      <td>0.418594</td>\n",
       "      <td>0.636438</td>\n",
       "      <td>green</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bone_length  rotting_flesh  hair_length  has_soul  color  type\n",
       "0   0     0.354512       0.350839     0.465761  0.781142  clear     2\n",
       "1   1     0.575560       0.425868     0.531401  0.439899  green     0\n",
       "2   2     0.467875       0.354330     0.811616  0.791225  black     2\n",
       "3   4     0.776652       0.508723     0.636766  0.884464  black     2\n",
       "4   5     0.566117       0.875862     0.418594  0.636438  green     1"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    129\n",
       "0    125\n",
       "1    117\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bone_length</th>\n",
       "      <th>rotting_flesh</th>\n",
       "      <th>hair_length</th>\n",
       "      <th>has_soul</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.471774</td>\n",
       "      <td>0.387937</td>\n",
       "      <td>0.706087</td>\n",
       "      <td>0.698537</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>0.427332</td>\n",
       "      <td>0.645024</td>\n",
       "      <td>0.565558</td>\n",
       "      <td>0.451462</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>0.549602</td>\n",
       "      <td>0.491931</td>\n",
       "      <td>0.660387</td>\n",
       "      <td>0.449809</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.682867</td>\n",
       "      <td>0.471409</td>\n",
       "      <td>0.356924</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.361762</td>\n",
       "      <td>0.583997</td>\n",
       "      <td>0.377256</td>\n",
       "      <td>0.276364</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bone_length  rotting_flesh  hair_length  has_soul  color\n",
       "0   3     0.471774       0.387937     0.706087  0.698537  black\n",
       "1   6     0.427332       0.645024     0.565558  0.451462  white\n",
       "2   9     0.549602       0.491931     0.660387  0.449809  black\n",
       "3  10     0.638095       0.682867     0.471409  0.356924  white\n",
       "4  13     0.361762       0.583997     0.377256  0.276364  black"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 6)\n",
      "(529, 6)\n"
     ]
    }
   ],
   "source": [
    "one_hot_train = pd.get_dummies(train.color, dtype='float')\n",
    "one_hot_test = pd.get_dummies(test.color, dtype='float')\n",
    "print(one_hot_train.shape)\n",
    "print(one_hot_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 13)\n",
      "(529, 12)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([train, one_hot_train], axis=1)\n",
    "test_df = pd.concat([test, one_hot_test], axis=1)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_df.copy()\n",
    "test_data = test_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(371, 11)\n",
      "(529, 10)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.drop(['id', 'color'], axis=1)\n",
    "test_data = test_data.drop(['id', 'color'], axis=1)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259, 10)\n",
      "(112, 10)\n",
      "(259,)\n",
      "(112,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "label = train_data['type'].values\n",
    "train = train_data.drop(['type'], axis=1)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, label, test_size=0.3)\n",
    "\n",
    "print(train_X.shape)\n",
    "print(valid_X.shape)\n",
    "print(train_y.shape)\n",
    "print(valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分类模型\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix--:\n",
      "[[24  5  6]\n",
      " [ 5 33  0]\n",
      " [ 8  2 29]]\n",
      "Precision---------: 0.7678571428571429\n",
      "Recall------------: 0.7678571428571429\n",
      "F1_score----------: 0.7678571428571429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "seed = 10\n",
    "lr = LogisticRegression(random_state=seed, \n",
    "                        C=30, \n",
    "                        class_weight='balanced', \n",
    "                        solver='newton-cg', \n",
    "                        multi_class='multinomial')  \n",
    "lr = lr.fit(train_X, train_y)\n",
    "lr_pred = lr.predict(valid_X)\n",
    "\n",
    "print('confusion_matrix--:')\n",
    "print(confusion_matrix(valid_y, lr_pred))\n",
    "print('Precision---------:', precision_score(valid_y, lr_pred, average='micro'))\n",
    "print('Recall------------:', recall_score(valid_y, lr_pred, average='micro'))\n",
    "print('F1_score----------:', f1_score(valid_y, lr_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix--:\n",
      "[[24  5  6]\n",
      " [ 5 33  0]\n",
      " [ 8  2 29]]\n",
      "Precision---------: 0.7678571428571429\n",
      "Recall------------: 0.7678571428571429\n",
      "F1_score----------: 0.7678571428571429\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, label, test_size=0.3, random_state=10)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn = knn.fit(train_X, train_y)\n",
    "knn = lr.fit(train_X, train_y)\n",
    "knn_pred = knn.predict(valid_X)\n",
    "\n",
    "print('confusion_matrix--:')\n",
    "print(confusion_matrix(valid_y, knn_pred))\n",
    "print('Precision---------:', precision_score(valid_y, knn_pred, average='micro'))\n",
    "print('Recall------------:', recall_score(valid_y, knn_pred, average='micro'))\n",
    "print('F1_score----------:', f1_score(valid_y, knn_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix--:\n",
      "[[24  5  6]\n",
      " [ 7 31  0]\n",
      " [ 8  3 28]]\n",
      "Precision---------: 0.7410714285714286\n",
      "Recall------------: 0.7410714285714286\n",
      "F1_score----------: 0.7410714285714286\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, label, test_size=0.3, random_state=10)\n",
    "svc = SVC(C=3, kernel='rbf', random_state=10)\n",
    "svc = svc.fit(train_X, train_y)\n",
    "svc_pred = svc.predict(valid_X)\n",
    "\n",
    "print('confusion_matrix--:')\n",
    "print(confusion_matrix(valid_y, svc_pred))\n",
    "print('Precision---------:', precision_score(valid_y, svc_pred, average='micro'))\n",
    "print('Recall------------:', recall_score(valid_y, svc_pred, average='micro'))\n",
    "print('F1_score----------:', f1_score(valid_y, svc_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion_matrix--:\n",
      "[[25  5  5]\n",
      " [ 7 31  0]\n",
      " [ 8  2 29]]\n",
      "Precision---------: 0.7589285714285714\n",
      "Recall------------: 0.7589285714285714\n",
      "F1_score----------: 0.7589285714285714\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, label, test_size=0.3, random_state=10)\n",
    "rfc = RFC(n_estimators=150, max_depth=4, random_state=10)\n",
    "rfc = rfc.fit(train_X, train_y)\n",
    "rfc_pred = rfc.predict(valid_X)\n",
    "\n",
    "print('confusion_matrix--:')\n",
    "print(confusion_matrix(valid_y, rfc_pred))\n",
    "print('Precision---------:', precision_score(valid_y, rfc_pred, average='micro'))\n",
    "print('Recall------------:', recall_score(valid_y, rfc_pred, average='micro'))\n",
    "print('F1_score----------:', f1_score(valid_y, rfc_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's multi_logloss: 0.388456\tvalid_1's multi_logloss: 0.598804\n",
      "[200]\ttraining's multi_logloss: 0.316302\tvalid_1's multi_logloss: 0.614812\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's multi_logloss: 0.408648\tvalid_1's multi_logloss: 0.589863\n",
      "confusion_matrix--:\n",
      "[[26  5  4]\n",
      " [ 6 32  0]\n",
      " [11  2 26]]\n",
      "Precision---------: 0.75\n",
      "Recall------------: 0.75\n",
      "F1_score----------: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "params = {'boosting_type':'gbdt',\n",
    "          'num_leaves': 60, \n",
    "          'min_data_in_leaf': 30,\n",
    "          'objective': 'multiclass',\n",
    "          'num_class': 3,\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.06,\n",
    "          \"min_sum_hessian_in_leaf\": 6,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"feature_fraction\": 0.9,\n",
    "          \"bagging_freq\": 1,\n",
    "          \"bagging_fraction\": 0.8,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"lambda_l1\": 0.4,\t\n",
    "          \"lambda_l2\": 0.5,\n",
    "          \"verbosity\": -1,\n",
    "          'metric': 'multi_logloss',\n",
    "          \"random_state\": 2022,\t\n",
    "          }\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, label, test_size=0.3, random_state=10)\n",
    "tr_data = lgb.Dataset(train_X, label=train_y)\n",
    "val_data = lgb.Dataset(valid_X, label=valid_y)\n",
    "num_round = 1000\n",
    "lgb = lgb.train(params, \n",
    "                tr_data,\n",
    "                num_round,\n",
    "                valid_sets=[tr_data, val_data],\n",
    "                verbose_eval=100,\n",
    "                early_stopping_rounds=200)\n",
    "y_pred = lgb.predict(valid_X, num_iteration=lgb.best_iteration)\n",
    "lgb_pred = [list(x).index(max(x)) for x in y_pred]\n",
    "\n",
    "print('confusion_matrix--:')\n",
    "print(confusion_matrix(valid_y, lgb_pred))\n",
    "print('Precision---------:', precision_score(valid_y, lgb_pred, average='micro'))\n",
    "print('Recall------------:', recall_score(valid_y, lgb_pred, average='micro'))\n",
    "print('F1_score----------:', f1_score(valid_y, lgb_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed----------------: 10\n",
      "Model: \"sequential_103\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_344 (Dense)           (None, 64)                704       \n",
      "                                                                 \n",
      " dense_345 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_346 (Dense)           (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,059\n",
      "Trainable params: 5,059\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 584ms/step - loss: 1.1109 - accuracy: 0.2934\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0906 - accuracy: 0.3514\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0798 - accuracy: 0.3475\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0712 - accuracy: 0.3475\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0637 - accuracy: 0.3629\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0565 - accuracy: 0.3707\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0496 - accuracy: 0.3861\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0429 - accuracy: 0.3977\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0363 - accuracy: 0.4208\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0299 - accuracy: 0.4595\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0236 - accuracy: 0.4981\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0173 - accuracy: 0.5174\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0107 - accuracy: 0.5405\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 1.0040 - accuracy: 0.5598\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9972 - accuracy: 0.5792\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9904 - accuracy: 0.5714\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9836 - accuracy: 0.5985\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.9767 - accuracy: 0.6100\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9698 - accuracy: 0.6178\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9628 - accuracy: 0.6216\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9557 - accuracy: 0.6409\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9486 - accuracy: 0.6448\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9415 - accuracy: 0.6641\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9343 - accuracy: 0.6525\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9270 - accuracy: 0.6680\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9196 - accuracy: 0.6564\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9121 - accuracy: 0.6757\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9045 - accuracy: 0.6795\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8968 - accuracy: 0.6834\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8890 - accuracy: 0.6834\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8810 - accuracy: 0.6873\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8729 - accuracy: 0.6873\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8646 - accuracy: 0.6834\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8559 - accuracy: 0.7066\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8471 - accuracy: 0.6873\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8383 - accuracy: 0.7027\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8297 - accuracy: 0.6911\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8212 - accuracy: 0.7181\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8129 - accuracy: 0.6873\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8046 - accuracy: 0.7220\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7964 - accuracy: 0.7066\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7883 - accuracy: 0.7259\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7803 - accuracy: 0.7143\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7724 - accuracy: 0.7336\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7646 - accuracy: 0.7336\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7572 - accuracy: 0.7375\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7500 - accuracy: 0.7375\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7431 - accuracy: 0.7336\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7365 - accuracy: 0.7375\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7300 - accuracy: 0.7490\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.7238 - accuracy: 0.7336\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7176 - accuracy: 0.7490\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7116 - accuracy: 0.7336\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7056 - accuracy: 0.7490\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6998 - accuracy: 0.7375\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6941 - accuracy: 0.7490\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6885 - accuracy: 0.7297\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6831 - accuracy: 0.7452\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6778 - accuracy: 0.7259\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6729 - accuracy: 0.7297\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6681 - accuracy: 0.7220\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6633 - accuracy: 0.7259\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6588 - accuracy: 0.7220\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6542 - accuracy: 0.7259\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6499 - accuracy: 0.7259\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6456 - accuracy: 0.7259\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6414 - accuracy: 0.7297\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6373 - accuracy: 0.7259\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6334 - accuracy: 0.7259\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6296 - accuracy: 0.7259\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6259 - accuracy: 0.7259\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6223 - accuracy: 0.7259\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6188 - accuracy: 0.7297\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6153 - accuracy: 0.7220\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6120 - accuracy: 0.7297\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6088 - accuracy: 0.7181\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6057 - accuracy: 0.7181\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6026 - accuracy: 0.7220\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5996 - accuracy: 0.7181\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5966 - accuracy: 0.7181\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5937 - accuracy: 0.7181\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5909 - accuracy: 0.7181\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5883 - accuracy: 0.7220\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5857 - accuracy: 0.7220\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5831 - accuracy: 0.7220\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5807 - accuracy: 0.7220\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5783 - accuracy: 0.7181\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5759 - accuracy: 0.7375\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5737 - accuracy: 0.7181\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5715 - accuracy: 0.7259\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5693 - accuracy: 0.7181\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5671 - accuracy: 0.7259\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5650 - accuracy: 0.7181\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5629 - accuracy: 0.7336\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5608 - accuracy: 0.7336\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5589 - accuracy: 0.7297\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5571 - accuracy: 0.7297\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5552 - accuracy: 0.7297\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5533 - accuracy: 0.7297\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5516 - accuracy: 0.7375\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5499 - accuracy: 0.7297\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5483 - accuracy: 0.7297\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5467 - accuracy: 0.7336\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5450 - accuracy: 0.7336\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5434 - accuracy: 0.7297\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5418 - accuracy: 0.7375\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5402 - accuracy: 0.7336\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5387 - accuracy: 0.7375\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5371 - accuracy: 0.7336\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5358 - accuracy: 0.7375\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5343 - accuracy: 0.7336\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5330 - accuracy: 0.7375\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5317 - accuracy: 0.7336\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5306 - accuracy: 0.7375\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5294 - accuracy: 0.7336\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5282 - accuracy: 0.7336\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5270 - accuracy: 0.7336\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5260 - accuracy: 0.7297\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5247 - accuracy: 0.7336\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5237 - accuracy: 0.7336\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5224 - accuracy: 0.7297\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5215 - accuracy: 0.7375\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5204 - accuracy: 0.7336\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5195 - accuracy: 0.7336\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5185 - accuracy: 0.7375\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5177 - accuracy: 0.7375\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5168 - accuracy: 0.7375\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5159 - accuracy: 0.7375\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5149 - accuracy: 0.7375\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5140 - accuracy: 0.7413\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5131 - accuracy: 0.7336\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5124 - accuracy: 0.7375\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5115 - accuracy: 0.7375\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5106 - accuracy: 0.7413\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5095 - accuracy: 0.7413\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5087 - accuracy: 0.7490\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5078 - accuracy: 0.7452\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5070 - accuracy: 0.7413\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5064 - accuracy: 0.7375\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5059 - accuracy: 0.7452\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5054 - accuracy: 0.7375\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5048 - accuracy: 0.7452\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5041 - accuracy: 0.7375\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5034 - accuracy: 0.7452\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5025 - accuracy: 0.7375\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5016 - accuracy: 0.7452\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5009 - accuracy: 0.7452\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5002 - accuracy: 0.7490\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4996 - accuracy: 0.7452\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4993 - accuracy: 0.7452\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4989 - accuracy: 0.7490\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4984 - accuracy: 0.7452\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4977 - accuracy: 0.7490\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4970 - accuracy: 0.7452\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4963 - accuracy: 0.7452\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4958 - accuracy: 0.7490\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4955 - accuracy: 0.7375\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4955 - accuracy: 0.7490\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4953 - accuracy: 0.7452\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4946 - accuracy: 0.7452\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4938 - accuracy: 0.7452\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4930 - accuracy: 0.7452\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4923 - accuracy: 0.7490\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4918 - accuracy: 0.7490\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4915 - accuracy: 0.7452\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4913 - accuracy: 0.7529\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4913 - accuracy: 0.7490\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4911 - accuracy: 0.7568\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4907 - accuracy: 0.7490\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4901 - accuracy: 0.7568\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4894 - accuracy: 0.7490\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4888 - accuracy: 0.7529\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4883 - accuracy: 0.7490\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4880 - accuracy: 0.7452\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4878 - accuracy: 0.7490\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4877 - accuracy: 0.7413\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4876 - accuracy: 0.7452\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4872 - accuracy: 0.7452\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4867 - accuracy: 0.7452\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4860 - accuracy: 0.7413\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4856 - accuracy: 0.7529\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4852 - accuracy: 0.7490\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4851 - accuracy: 0.7490\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4852 - accuracy: 0.7606\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4852 - accuracy: 0.7529\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4849 - accuracy: 0.7606\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4843 - accuracy: 0.7529\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4835 - accuracy: 0.7568\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4829 - accuracy: 0.7490\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4825 - accuracy: 0.7490\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4823 - accuracy: 0.7529\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4822 - accuracy: 0.7452\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4825 - accuracy: 0.7490\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4825 - accuracy: 0.7452\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4822 - accuracy: 0.7490\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4816 - accuracy: 0.7490\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4810 - accuracy: 0.7490\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4805 - accuracy: 0.7490\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4803 - accuracy: 0.7606\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4804 - accuracy: 0.7529\n",
      "confusion_matrix--:\n",
      "[[26  5  6]\n",
      " [ 3 33  0]\n",
      " [ 6  0 33]]\n",
      "Precision---------: 0.8214285714285714\n",
      "Recall------------: 0.8214285714285714\n",
      "F1_score----------: 0.8214285714285714\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "seed = 10\n",
    "print('seed----------------:', seed)\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, label, test_size=0.3, random_state=10)\n",
    "\n",
    "X_train_NN = (train_X.values).astype('float32') # all pixel values\n",
    "y_train_NN = train_y.astype('int32')\n",
    "X_valid_NN = (valid_X.values).astype('float32')\n",
    "y_valid_NN = valid_y.astype('int32')\n",
    "\n",
    "one_hot_train_y=to_categorical(train_y)\n",
    "one_hot_valid_y=to_categorical(valid_y)\n",
    "\n",
    "#-----------------------------------------构建为网络\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "input_shape = X_train_NN.shape[1]\n",
    "b_size = 500\n",
    "max_epochs = 200\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64,activation='relu',input_shape = (10,)))\n",
    "# model.add(layers.Dense(128,activation='relu'))\n",
    "# model.add(layers.Dense(128,activation='relu'))\n",
    "model.add(layers.Dense(64,activation='relu'))\n",
    "model.add(layers.Dense(3,activation='softmax'))\n",
    "print(model.summary())\n",
    "#------------------------------------------训练模型\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss=\"categorical_crossentropy\", \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "h = model.fit(X_train_NN, \n",
    "              one_hot_train_y, \n",
    "              batch_size=b_size, \n",
    "              epochs=max_epochs, \n",
    "              shuffle=True, \n",
    "              verbose=1)\n",
    "nn_pred = model.predict(X_valid_NN)\n",
    "\n",
    "NN_pred = []\n",
    "for v in nn_pred:\n",
    "    index = np.argmax(v)\n",
    "    NN_pred.append(index)\n",
    "\n",
    "print('confusion_matrix--:')\n",
    "print(confusion_matrix(NN_pred, y_valid_NN))\n",
    "print('Precision---------:', precision_score(y_valid_NN, NN_pred, average='micro'))\n",
    "print('Recall------------:', recall_score(y_valid_NN, NN_pred, average='micro'))\n",
    "print('F1_score----------:', f1_score(y_valid_NN, NN_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动化寻找最佳模型\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "import lightgbm as lgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "seed = 10\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, label, test_size=0.3, random_state=seed)\n",
    "\n",
    "def LR():\n",
    "    seed = 10\n",
    "    lr = LogisticRegression(random_state=seed, \n",
    "                            C=30, \n",
    "                            class_weight='balanced', \n",
    "                            solver='newton-cg', \n",
    "                            multi_class='multinomial')  \n",
    "    lr = lr.fit(train_X, train_y)\n",
    "    lr_pred = lr.predict(valid_X)\n",
    "    \n",
    "    return [precision_score(valid_y, lr_pred, average='micro'),\n",
    "            recall_score(valid_y, lr_pred, average='micro'), \n",
    "            f1_score(valid_y, lr_pred, average='micro')]\n",
    "\n",
    "def KNN():\n",
    "    knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    knn = knn.fit(train_X, train_y)\n",
    "    knn = lr.fit(train_X, train_y)\n",
    "    knn_pred = knn.predict(valid_X)\n",
    "\n",
    "    return [precision_score(valid_y, knn_pred, average='micro'),\n",
    "            recall_score(valid_y, knn_pred, average='micro'), \n",
    "            f1_score(valid_y, knn_pred, average='micro')]\n",
    "    \n",
    "def SVM():\n",
    "    svc = SVC(C=3, kernel='rbf', random_state=seed)\n",
    "    svc = svc.fit(train_X, train_y)\n",
    "    svc_pred = svc.predict(valid_X)\n",
    "\n",
    "    return [precision_score(valid_y, svc_pred, average='micro'),\n",
    "            recall_score(valid_y, svc_pred, average='micro'), \n",
    "            f1_score(valid_y, svc_pred, average='micro')]\n",
    "\n",
    "def rfc():\n",
    "    rfc = RFC(n_estimators=150, max_depth=4, random_state=seed)\n",
    "    rfc = rfc.fit(train_X, train_y)\n",
    "    rfc_pred = rfc.predict(valid_X)\n",
    "\n",
    "    return [precision_score(valid_y, rfc_pred, average='micro'),\n",
    "            recall_score(valid_y, rfc_pred, average='micro'), \n",
    "            f1_score(valid_y, rfc_pred, average='micro')]\n",
    "    \n",
    "def lgb_model():\n",
    "    params = {'boosting_type':'gbdt',\n",
    "          'num_leaves': 60, \n",
    "          'min_data_in_leaf': 30,\n",
    "          'objective': 'multiclass',\n",
    "          'num_class': 3,\n",
    "          'max_depth': -1,\n",
    "          'learning_rate': 0.06,\n",
    "          \"min_sum_hessian_in_leaf\": 6,\n",
    "          \"boosting\": \"gbdt\",\n",
    "          \"feature_fraction\": 0.9,\n",
    "          \"bagging_freq\": 1,\n",
    "          \"bagging_fraction\": 0.8,\n",
    "          \"bagging_seed\": 11,\n",
    "          \"lambda_l1\": 0.4,\t\n",
    "          \"lambda_l2\": 0.5,\n",
    "          \"verbosity\": -1,\t\t\t\t\n",
    "          'metric': 'multi_logloss',\t\n",
    "          \"random_state\": 2022,\t\n",
    "          }\n",
    "    tr_data = lgb.Dataset(train_X, label=train_y)\n",
    "    val_data = lgb.Dataset(valid_X, label=valid_y)\n",
    "    num_round = 1000\n",
    "    lgb_model = lgb.train(params, \n",
    "                    tr_data,\n",
    "                    num_round,\n",
    "                    valid_sets=[tr_data, val_data],\n",
    "                    verbose_eval=100,\n",
    "                    early_stopping_rounds=200)\n",
    "    y_pred = lgb_model.predict(valid_X, num_iteration=lgb_model.best_iteration)\n",
    "    lgb_pred = [list(x).index(max(x)) for x in y_pred]\n",
    "\n",
    "    return [precision_score(valid_y, lgb_pred, average='micro'),\n",
    "            recall_score(valid_y, lgb_pred, average='micro'), \n",
    "            f1_score(valid_y, lgb_pred, average='micro')]\n",
    "    \n",
    "    \n",
    "def NN():\n",
    "    X_train_NN = (train_X.values).astype('float32') # all pixel values\n",
    "    y_train_NN = train_y.astype('int32')\n",
    "    X_valid_NN = (valid_X.values).astype('float32')\n",
    "    y_valid_NN = valid_y.astype('int32')\n",
    "\n",
    "    one_hot_train_y=to_categorical(train_y)\n",
    "    one_hot_valid_y=to_categorical(valid_y)\n",
    "\n",
    "    #-----------------------------------------构建为网络\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    input_shape = X_train_NN.shape[1]\n",
    "    b_size = 500\n",
    "    max_epochs = 200\n",
    "    \n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64,activation='relu',input_shape = (10,)))\n",
    "    model.add(layers.Dense(64,activation='relu'))\n",
    "    model.add(layers.Dense(3,activation='softmax'))\n",
    "    print(model.summary())\n",
    "    #------------------------------------------训练模型\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss=\"categorical_crossentropy\", \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    h = model.fit(X_train_NN, \n",
    "                  one_hot_train_y, \n",
    "                  batch_size=b_size, \n",
    "                  epochs=max_epochs, \n",
    "                  shuffle=True, \n",
    "                  verbose=1)\n",
    "    nn_pred = model.predict(X_valid_NN)\n",
    "\n",
    "    NN_pred = []\n",
    "    for v in nn_pred:\n",
    "        index = np.argmax(v)\n",
    "        NN_pred.append(index)\n",
    "\n",
    "    return [precision_score(valid_y, NN_pred, average='micro'),\n",
    "            recall_score(valid_y, NN_pred, average='micro'), \n",
    "            f1_score(valid_y, NN_pred, average='micro')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] boosting is set=gbdt, boosting_type=gbdt will be ignored. Current value: boosting=gbdt\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\ttraining's multi_logloss: 0.388456\tvalid_1's multi_logloss: 0.598804\n",
      "[200]\ttraining's multi_logloss: 0.316302\tvalid_1's multi_logloss: 0.614812\n",
      "Early stopping, best iteration is:\n",
      "[82]\ttraining's multi_logloss: 0.408648\tvalid_1's multi_logloss: 0.589863\n",
      "Model: \"sequential_108\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:181: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "D:\\Anaconda\\lib\\site-packages\\lightgbm\\engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_359 (Dense)           (None, 64)                704       \n",
      "                                                                 \n",
      " dense_360 (Dense)           (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_361 (Dense)           (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,059\n",
      "Trainable params: 5,059\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 614ms/step - loss: 1.1109 - accuracy: 0.2934\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0906 - accuracy: 0.3514\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0798 - accuracy: 0.3475\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0712 - accuracy: 0.3475\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.0637 - accuracy: 0.3629\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0565 - accuracy: 0.3707\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0496 - accuracy: 0.3861\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0429 - accuracy: 0.3977\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0363 - accuracy: 0.4208\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0299 - accuracy: 0.4595\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0236 - accuracy: 0.4981\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 1.0173 - accuracy: 0.5174\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.0107 - accuracy: 0.5405\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.0040 - accuracy: 0.5598\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9972 - accuracy: 0.5792\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9904 - accuracy: 0.5714\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9836 - accuracy: 0.5985\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9767 - accuracy: 0.6100\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9698 - accuracy: 0.6178\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9628 - accuracy: 0.6216\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9557 - accuracy: 0.6409\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9486 - accuracy: 0.6448\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9415 - accuracy: 0.6641\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.9343 - accuracy: 0.6525\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9270 - accuracy: 0.6680\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.9196 - accuracy: 0.6564\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.9121 - accuracy: 0.6757\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.9045 - accuracy: 0.6795\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8968 - accuracy: 0.6834\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8890 - accuracy: 0.6834\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8810 - accuracy: 0.6873\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8729 - accuracy: 0.6873\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.8646 - accuracy: 0.6834\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8559 - accuracy: 0.7066\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8471 - accuracy: 0.6873\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8383 - accuracy: 0.7027\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8297 - accuracy: 0.6911\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.8212 - accuracy: 0.7181\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.8129 - accuracy: 0.6873\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.8046 - accuracy: 0.7220\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7964 - accuracy: 0.7066\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7883 - accuracy: 0.7259\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7803 - accuracy: 0.7143\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7724 - accuracy: 0.7336\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7646 - accuracy: 0.7336\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7572 - accuracy: 0.7375\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7500 - accuracy: 0.7375\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7431 - accuracy: 0.7336\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7365 - accuracy: 0.7375\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7300 - accuracy: 0.7490\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7238 - accuracy: 0.7336\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.7176 - accuracy: 0.7490\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7116 - accuracy: 0.7336\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7056 - accuracy: 0.7490\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6998 - accuracy: 0.7375\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6941 - accuracy: 0.7490\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6885 - accuracy: 0.7297\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6831 - accuracy: 0.7452\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6778 - accuracy: 0.7259\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6729 - accuracy: 0.7297\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6681 - accuracy: 0.7220\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6633 - accuracy: 0.7259\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6588 - accuracy: 0.7220\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6542 - accuracy: 0.7259\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6499 - accuracy: 0.7259\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6456 - accuracy: 0.7259\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6414 - accuracy: 0.7297\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6373 - accuracy: 0.7259\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6334 - accuracy: 0.7259\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6296 - accuracy: 0.7259\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6259 - accuracy: 0.7259\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6223 - accuracy: 0.7259\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6188 - accuracy: 0.7297\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6153 - accuracy: 0.7220\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6120 - accuracy: 0.7297\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6088 - accuracy: 0.7181\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6057 - accuracy: 0.7181\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6026 - accuracy: 0.7220\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5996 - accuracy: 0.7181\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5966 - accuracy: 0.7181\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5937 - accuracy: 0.7181\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5909 - accuracy: 0.7181\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5883 - accuracy: 0.7220\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5857 - accuracy: 0.7220\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5831 - accuracy: 0.7220\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5807 - accuracy: 0.7220\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5783 - accuracy: 0.7181\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5759 - accuracy: 0.7375\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5737 - accuracy: 0.7181\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5715 - accuracy: 0.7259\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5693 - accuracy: 0.7181\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5671 - accuracy: 0.7259\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5650 - accuracy: 0.7181\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5629 - accuracy: 0.7336\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5608 - accuracy: 0.7336\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5589 - accuracy: 0.7297\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5571 - accuracy: 0.7297\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5552 - accuracy: 0.7297\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5533 - accuracy: 0.7297\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5516 - accuracy: 0.7375\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5499 - accuracy: 0.7297\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5483 - accuracy: 0.7297\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5467 - accuracy: 0.7336\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5450 - accuracy: 0.7336\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5434 - accuracy: 0.7297\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5418 - accuracy: 0.7375\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5402 - accuracy: 0.7336\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5387 - accuracy: 0.7375\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5371 - accuracy: 0.7336\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5358 - accuracy: 0.7375\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5343 - accuracy: 0.7336\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5330 - accuracy: 0.7375\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5317 - accuracy: 0.7336\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5306 - accuracy: 0.7375\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5294 - accuracy: 0.7336\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5282 - accuracy: 0.7336\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5270 - accuracy: 0.7336\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5260 - accuracy: 0.7297\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5247 - accuracy: 0.7336\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5237 - accuracy: 0.7336\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5224 - accuracy: 0.7297\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5215 - accuracy: 0.7375\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5204 - accuracy: 0.7336\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5195 - accuracy: 0.7336\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5185 - accuracy: 0.7375\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5177 - accuracy: 0.7375\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5168 - accuracy: 0.7375\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5159 - accuracy: 0.7375\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5149 - accuracy: 0.7375\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5140 - accuracy: 0.7413\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5131 - accuracy: 0.7336\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5124 - accuracy: 0.7375\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5115 - accuracy: 0.7375\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5106 - accuracy: 0.7413\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5095 - accuracy: 0.7413\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5087 - accuracy: 0.7490\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5078 - accuracy: 0.7452\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5070 - accuracy: 0.7413\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5064 - accuracy: 0.7375\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5059 - accuracy: 0.7452\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5054 - accuracy: 0.7375\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5048 - accuracy: 0.7452\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.5041 - accuracy: 0.7375\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5034 - accuracy: 0.7452\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5025 - accuracy: 0.7375\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5016 - accuracy: 0.7452\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.5009 - accuracy: 0.7452\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.5002 - accuracy: 0.7490\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4996 - accuracy: 0.7452\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4993 - accuracy: 0.7452\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4989 - accuracy: 0.7490\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4984 - accuracy: 0.7452\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4977 - accuracy: 0.7490\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4970 - accuracy: 0.7452\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4963 - accuracy: 0.7452\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4958 - accuracy: 0.7490\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4955 - accuracy: 0.7375\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4955 - accuracy: 0.7490\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4953 - accuracy: 0.7452\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4946 - accuracy: 0.7452\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4938 - accuracy: 0.7452\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4930 - accuracy: 0.7452\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4923 - accuracy: 0.7490\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4918 - accuracy: 0.7490\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4915 - accuracy: 0.7452\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4913 - accuracy: 0.7529\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4913 - accuracy: 0.7490\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4911 - accuracy: 0.7568\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4907 - accuracy: 0.7490\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4901 - accuracy: 0.7568\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4894 - accuracy: 0.7490\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4888 - accuracy: 0.7529\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4883 - accuracy: 0.7490\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4880 - accuracy: 0.7452\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4878 - accuracy: 0.7490\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4877 - accuracy: 0.7413\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4876 - accuracy: 0.7452\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4872 - accuracy: 0.7452\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4867 - accuracy: 0.7452\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.4860 - accuracy: 0.7413\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4856 - accuracy: 0.7529\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4852 - accuracy: 0.7490\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4851 - accuracy: 0.7490\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4852 - accuracy: 0.7606\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4852 - accuracy: 0.7529\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4849 - accuracy: 0.7606\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4843 - accuracy: 0.7529\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4835 - accuracy: 0.7568\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4829 - accuracy: 0.7490\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4825 - accuracy: 0.7490\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4823 - accuracy: 0.7529\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4822 - accuracy: 0.7452\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4825 - accuracy: 0.7490\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4825 - accuracy: 0.7452\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4822 - accuracy: 0.7490\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4816 - accuracy: 0.7490\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4810 - accuracy: 0.7490\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.4805 - accuracy: 0.7490\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4803 - accuracy: 0.7606\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4804 - accuracy: 0.7529\n",
      "Pre最高的模型是---------： NN()\n",
      "Recall最高的模型是---------： NN()\n",
      "F1_score最高的模型是-------： NN()\n"
     ]
    }
   ],
   "source": [
    "model_dict = {'0':'LR()', \n",
    "              '1':'KNN()', \n",
    "              '2':'SVM()', \n",
    "              '3':'rfc()', \n",
    "              '4':'lgb_model()', \n",
    "              '5':'NN()'}\n",
    "MODELS = [LR(), KNN(), SVM(), rfc(), lgb_model(), NN()]\n",
    "Pre = []\n",
    "Recall = []\n",
    "F1_score = []\n",
    "for MODEL in MODELS:\n",
    "    clf = MODEL\n",
    "    Pre.append(clf[0])\n",
    "    Recall.append(clf[1])\n",
    "    F1_score.append(clf[2])\n",
    "Pre_index = np.argmax(Pre)\n",
    "Recall_index = np.argmax(Recall)\n",
    "F1_score_index = np.argmax(F1_score)\n",
    "\n",
    "print('Pre最高的模型是---------：', model_dict[str(Pre_index)])\n",
    "print('Recall最高的模型是---------：', model_dict[str(Recall_index)])\n",
    "print('F1_score最高的模型是-------：', model_dict[str(F1_score_index)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "seed = 10\n",
    "train_X, valid_X, train_y, valid_y = train_test_split(train, label, test_size=0.3, random_state=seed)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(128, 30), \n",
    "                    activation='relu', \n",
    "                    solver='adam', \n",
    "                    max_iter=200, \n",
    "                    random_state=seed)\n",
    "\n",
    "X_train_NN = (train_X.values).astype('float32')\n",
    "y_train_NN = train_y.astype('int32')\n",
    "X_valid_NN = (valid_X.values).astype('float32')\n",
    "y_valid_NN = valid_y.astype('int32')\n",
    "\n",
    "mlp = mlp.fit(X_train_NN, y_train_NN)\n",
    "y_pred = mlp.predict(X_valid_NN) \n",
    "\n",
    "print('confusion_matrix--:')\n",
    "print(confusion_matrix(y_pred, y_valid_NN))\n",
    "print('Precision---------:', precision_score(y_valid_NN, y_pred, average='micro'))\n",
    "print('Recall------------:', recall_score(y_valid_NN, y_pred, average='micro'))\n",
    "print('F1_score----------:', f1_score(y_valid_NN, y_pred, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
