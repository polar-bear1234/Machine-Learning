{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sSo8VC5rjcd"
   },
   "source": [
    "# AI Project\n",
    "***\n",
    "## Administration and Rules:\n",
    "\n",
    "Greetings, this is the first project in our project series. Generally, projects are larger problem sets, and contain more open-ended questions. You will be given some general guidelines and are allowed to explore more freely.\n",
    "\n",
    "* **Guideline** In your submitted version\n",
    "    * This project is divided into 5 parts.\n",
    "        * For each part, you will be given a separate deadline.\n",
    "        * Upon the deadline of each part, a sample analysis of this previous part will be released for you as your starting point for the next parts. (Think of this as Checkpoints in gaming)\n",
    "        * So, start early!\n",
    "\n",
    "    \n",
    "* **How to submit:**\n",
    "    * Submit on Google Classroom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tz8i_Mearjci"
   },
   "source": [
    "***\n",
    "## Introduction\n",
    "In this project, you will be using a dataset called \"Ghouls Goblins and Ghosts\"\n",
    "\n",
    "### Context\n",
    "After a month of making scientific observations and taking careful measurements, we’ve determined that 900 ghouls, ghosts, and goblins are infesting our halls and frightening our fellow teachers and students here at Pinghe School. When trying garlic, asking politely, and using reverse psychology didn't work, it became clear that machine learning is the only answer to banishing our unwanted guests.\n",
    "\n",
    "![halloween-660x.png](https://i.loli.net/2020/05/24/WeOE4TFNJp3DQnm.png)\n",
    "\n",
    "So now the hour has come to put the data we’ve collected in your hands. We’ve managed to identify 371 of the ghastly creatures, but need your help to vanquish the rest. And only an accurate classification algorithm can thwart them. Use bone length measurements, severity of rot, extent of soullessness, and other characteristics to distinguish (and extinguish) the intruders. Are you ghost-busters up for the challenge?\n",
    "\n",
    "### Dataset\n",
    "Dataset file:\n",
    "* Train: https://drive.google.com/file/d/1A7kgIjEruZv3qWbNl5kWzP5xRQbZmou5/view?usp=sharing\n",
    "* Test: https://drive.google.com/file/d/1UhYjXWwH5L4BzUPWB9Iq5wO05_2SyMdm/view?usp=sharing\n",
    "\n",
    "File descriptions\n",
    "* `train.csv` - the training set, which contains both features and labels (target variables)\n",
    "* `test.csv` - the test set, which contains only features and your job is to predict the types\n",
    "\n",
    "Data fields\n",
    "* id - id of the creature\n",
    "* bone_length - average length of bone in the creature, normalized between 0 and 1\n",
    "* rotting_flesh - percentage of rotting flesh in the creature\n",
    "* hair_length - average hair length, normalized between 0 and 1\n",
    "* has_soul - percentage of soul in the creature\n",
    "* color - dominant color of the creature: 'white','black','clear','blue','green','blood'\n",
    "* type - target variable: 'Ghost', 'Goblin', and 'Ghoul'\n",
    "\n",
    "### Goal\n",
    "Predict the types of the spooky creatures!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qs9rL8pRdEeo"
   },
   "source": [
    "***\n",
    "## Project Milestone 0 - Starter\n",
    "\n",
    "Make sure you can run this starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lf7xWJfPdRPV"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5o038XXd0dp"
   },
   "source": [
    "In google colab, loading data is a little bit different, we need to first mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fs1YoazpddRZ",
    "outputId": "e52c3856-c552-4411-cf36-3bdd649defaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Mount the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hczkcUhMd-2D"
   },
   "source": [
    "Save a shortcut copy of the dataset onto your own drive\n",
    "* Data: https://drive.google.com/file/d/1A7kgIjEruZv3qWbNl5kWzP5xRQbZmou5/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "KxEQlieveMF9",
    "outputId": "75716f19-fe2f-4d90-ae92-cd6ed8754f07"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bone_length</th>\n",
       "      <th>rotting_flesh</th>\n",
       "      <th>hair_length</th>\n",
       "      <th>has_soul</th>\n",
       "      <th>color</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.354512</td>\n",
       "      <td>0.350839</td>\n",
       "      <td>0.465761</td>\n",
       "      <td>0.781142</td>\n",
       "      <td>clear</td>\n",
       "      <td>Ghoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.575560</td>\n",
       "      <td>0.425868</td>\n",
       "      <td>0.531401</td>\n",
       "      <td>0.439899</td>\n",
       "      <td>green</td>\n",
       "      <td>Goblin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.467875</td>\n",
       "      <td>0.354330</td>\n",
       "      <td>0.811616</td>\n",
       "      <td>0.791225</td>\n",
       "      <td>black</td>\n",
       "      <td>Ghoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.776652</td>\n",
       "      <td>0.508723</td>\n",
       "      <td>0.636766</td>\n",
       "      <td>0.884464</td>\n",
       "      <td>black</td>\n",
       "      <td>Ghoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.566117</td>\n",
       "      <td>0.875862</td>\n",
       "      <td>0.418594</td>\n",
       "      <td>0.636438</td>\n",
       "      <td>green</td>\n",
       "      <td>Ghost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bone_length  rotting_flesh  hair_length  has_soul  color    type\n",
       "0   0     0.354512       0.350839     0.465761  0.781142  clear   Ghoul\n",
       "1   1     0.575560       0.425868     0.531401  0.439899  green  Goblin\n",
       "2   2     0.467875       0.354330     0.811616  0.791225  black   Ghoul\n",
       "3   4     0.776652       0.508723     0.636766  0.884464  black   Ghoul\n",
       "4   5     0.566117       0.875862     0.418594  0.636438  green   Ghost"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9H2LG68fElc"
   },
   "source": [
    "## Congratulations to have successfully run the Project Starter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTjjrJ9drjcj"
   },
   "source": [
    "***\n",
    "## Project Milestone #1 (31 points)\n",
    "\n",
    "\n",
    "This is the first portion of the project, you are asked to explore the dataset using the tools we have learned in class so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nb3TH8irjck"
   },
   "source": [
    "### Question 1.1 (7 points)\n",
    "Get started:\n",
    "* Import the libraries **(2 point)**\n",
    "* Import the data: both the training set and the test set **(2 point)**\n",
    "* Take a quick look at the dataset, are there any missing values? **(3 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "faGhX2aJrjck"
   },
   "outputs": [],
   "source": [
    "# Import the libraries, expand the list as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9u4bBeurMFA"
   },
   "source": [
    "#### Import the training set**(1 point)**\n",
    "Dataset file:\n",
    "* Train: https://drive.google.com/file/d/1A7kgIjEruZv3qWbNl5kWzP5xRQbZmou5/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wtw77rnhujBr"
   },
   "outputs": [],
   "source": [
    "# Mount the drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sE9lNak5rjcl"
   },
   "outputs": [],
   "source": [
    "# Read in the training set\n",
    "\n",
    "\n",
    "# print the shapes of the training set\n",
    "\n",
    "\n",
    "# show the head of the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tooi9sQlrjcm"
   },
   "outputs": [],
   "source": [
    "# Let's check if there are any missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1A8RxGKrjcm"
   },
   "source": [
    "**Answer:** Our data set does not have any missing nor null/na values. We are good to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50WmD36Grjcr"
   },
   "source": [
    "### Question 1.2 **(12 points)**\n",
    "* What types of data are the features? Which are Quantitative and which are Qualitative? **(2 points)**\n",
    "* For the qualitative feature(s), perform a One-hot-encoding to transform it into separate columns for each category. Augment these new columns into the your `DataFrame` as new features. **(6 points)**\n",
    "* Drop features that you think are either irrelavant or redundant and store features and types into separate variables (for both train and test set), so 3 variables in total (you do not observe targets for the test set). **(4 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwz6WH2trjcr"
   },
   "source": [
    "#### What types of data are the features? Which are Quantitative and which are Qualitative? **(2 points)**\n",
    "\n",
    "**Answer:** From our previous exploration, the `Color` variable is categorical while the other features are all quantitative. Next, let's encode the `'Color'` feature before visualization. Right now, we will use a LabelEncoder to change the colors from text to integers of 0, 1, 2, 3, 4, 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR13LQ-usgme"
   },
   "source": [
    "#### For the qualitative feature(s), perform a One-hot-encoding to transform it into separate columns for each category. Augment these new columns into the your `DataFrame` as new features. **(6 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rucq7BRfrjcs"
   },
   "outputs": [],
   "source": [
    "# Let's do a one-hot encoding for the Color variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MJFxObWv-j7"
   },
   "outputs": [],
   "source": [
    "# use pd.concat to get augmented DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gCB3v5vzrjcs"
   },
   "source": [
    "**Answer:** we have finished one-hot-encoding for the `Color` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVka9t9stGLK"
   },
   "source": [
    "####  Drop features that you think are either irrelavant or redundant and store features and types into separate variables, so you should have two variables ready `train_features` and `train_target`. **(4 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb6Ri_tQrjct"
   },
   "outputs": [],
   "source": [
    "# First define your target variable y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuMyXguYwjLs"
   },
   "source": [
    "Your training features `train_features` should look like this\n",
    "![image.png](https://i.loli.net/2021/04/29/PjbAqUVOzvcRDX2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgvAsWmdrjct"
   },
   "outputs": [],
   "source": [
    "# Let's drop the 'id' and 'color' column as it doesn't help our classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mqs4gEXOrjct"
   },
   "source": [
    "**Answer:**\n",
    "* We have finished processing our feature variables for the training set.\n",
    "* We have also stored training targets into a separate variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO7Q_SCIrjct"
   },
   "source": [
    "### Question 1.3 **(12 points)**\n",
    "In this part of the project, let's treat this as separate binary classification problems first. Suppose that now we are only interested in identifying all the `Ghosts`. Then effectively, we can think of the problem as having just two classes: Ghosts and Non-Ghosts.\n",
    "\n",
    "Repeat this process for each type and we effectively have done the so-called One-vs-all multi-class classification method.\n",
    "\n",
    "\n",
    "Let's first perform a Logistic Regression for the `Ghost` vs `Other` problem. Follow the steps:\n",
    "* Transform your target variables into 0s and 1s - 1 for `Ghost`, 0 for `Other` **(4 Points)**\n",
    "* Run a Logistic Regression **(8 Points)**\n",
    "    * Pick accuracy as the model evaluation metrics\n",
    "    * Construct a validation set / method, explain why you pick this validation method.\n",
    "    * Calculate and output the evaluation metrics on the validation set / method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeo161ivyBs7"
   },
   "source": [
    "#### Transform your target variables into 0s and 1s - 1 for `Ghost`, 0 for `Other` **(4 Points)**\n",
    "\n",
    "Make a new target variable called `train_target_ghost`, where\n",
    "* 1: if type = 'Ghost'\n",
    "* 0: otherwise\n",
    "\n",
    "**Hint: you can use Selection method in Pandas to achieve this**\n",
    "\n",
    "Your `train_target_ghost` should look like this:\n",
    "\n",
    "![image.png](https://i.loli.net/2021/04/29/gAhWN1rVEToxkDX.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRkQN1LIrjct"
   },
   "outputs": [],
   "source": [
    "# Next we need to process the target variables.\n",
    "# Note that we now only care about whether the creature is a Ghost or not - Binary Classification\n",
    "# Hint: you can use Selection method in Pandas to achieve this\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiLYpUwirjcu"
   },
   "source": [
    "**Answer:** we have transformed targets into 0 and 1 with `Ghost` being the positive case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0URwwYL8t9dC"
   },
   "source": [
    "#### Split the training set into training vs validation set using the train_test split method\n",
    "\n",
    "You should have 4 variables ready:\n",
    "* X_train\n",
    "* X_valid\n",
    "* y_train_ghost\n",
    "* y_valid_ghost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwXuito0uGZ3"
   },
   "outputs": [],
   "source": [
    "# split the dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKJPjAK6uMbJ"
   },
   "source": [
    "#### Run a Logistic Regression **(8 Points)**\n",
    "* Pick accuracy as the model evaluation metrics\n",
    "* Calculate and output the evaluation metrics on the validation set / method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8FYh9ZIerjcu"
   },
   "outputs": [],
   "source": [
    "# Run a standard logistic regression\n",
    "\n",
    "\n",
    "\n",
    "# Make model predictions on the validation set\n",
    "\n",
    "\n",
    "\n",
    "# Let's pick accuracy for our metrics\n",
    "# calculate accuracy on the validation data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwZYpBO1rjcv"
   },
   "source": [
    "## Project Milestone #2 **(30 points)**\n",
    "\n",
    "\n",
    "In this part of the project, let's treat this as separate binary classification problems first. Suppose that now we are only interested in identifying all the `Ghosts`. Then effectively, we can think of the problem as having just two classes: Ghosts and Non-Ghosts.\n",
    "\n",
    "Repeat this process for each type and we effectively have done the so-called One-vs-all multi-class classification method. Let's explore this in Milestone #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqpCbBoFk32g"
   },
   "source": [
    "## 2.1 One-vs-all method (30 points)\n",
    "In this part, finish up the one-vs-all method. (Same as in 2.1, if you have already finished 2.1, you can simply copy your code from there)\n",
    "\n",
    "\n",
    "Here is an outline of what you need to do in this part:\n",
    "\n",
    "\n",
    "1. Split your data into train and validation\n",
    "2. Run 3 separate Logistic Regression models on the following Binary Classification Problems\n",
    "  * Ghost-vs-all\n",
    "  * Ghoul-vs-all\n",
    "  * Goblin-vs-all\n",
    "3. For each validation sample\n",
    "  * Predict probabilities using the 3 Logistic Regression models above (you can use the function model.predict_proba, which will return 2 columns of probabilities, column 0 for Class 0 and column 1 for Class 1)\n",
    "  * For each sample row, you will need to make classification given the 3 probablities you got (Classification means: you need to decide whether the creature is Ghost, Ghoul or Goblin)\n",
    "4. After you have your predicted classes for all the validation data, calculate the accuracy of the One-vs-all model\n",
    "  * You will need 2 columns - your predicted class obtained in step 3 above, and the 'correct answers' \n",
    "  * Comparing the 2 columns and calculate your model accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsad03M9iQzO"
   },
   "outputs": [],
   "source": [
    "# One-vs-all method code\n",
    "# You can add as many cells as you like\n",
    "# This is probably the harded part of the entire project\n",
    "# There are a lot of intermediate steps\n",
    "# Check your variables frequently and make sure you are on the right track before moving on\n",
    "# Your final output of this part is the accuracy (a number) of the One-vs-all model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLoQ5VBpjtcM"
   },
   "source": [
    "---\n",
    "## Project Milestone #3 **(20 points)**\n",
    "\n",
    "\n",
    "This is our final Milestone, here is an outline of what you need to do:\n",
    "\n",
    "\n",
    "1. (Optional) You can try building a different model, say a Neural Network\n",
    "2. Compare your models' performance on the validation set, and pick the best performing model.\n",
    "3. Use your best model to make final prediction on the test set and submit the submission file\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUJwMkTvw2B-"
   },
   "source": [
    "## (Optional) Building Additional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f408X7ew6ZQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53doQpKYw62X"
   },
   "source": [
    "## (Optional) Comparing Different Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GbueCd8ew_hT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "smXUbeEOrSuz"
   },
   "source": [
    "## Final submission (20 points)\n",
    "Whoo-hoo! Finally, we are ready to submit our predictions on the test set!\n",
    "\n",
    "Here is an outline of what you need to do in this part:\n",
    "1. Comparing the performance of all your models and pick the best one\n",
    "2. Train your best model one last time on the entire training+validation set using optimal settings\n",
    "3. Load the test.csv file: https://drive.google.com/file/d/1UhYjXWwH5L4BzUPWB9Iq5wO05_2SyMdm/view?usp=sharing\n",
    "4. Make final prediction on the test set\n",
    "  * Make sure you need to do all the transformation on the test set:\n",
    "    * One-hot-encoding\n",
    "    * Selecting the same variables\n",
    "    * etc\n",
    "5. Your final output is a Pandas Dataframe\n",
    "  * It should have only 2 columns: id, type\n",
    "  * For each row in the test set\n",
    "    * Keep the id and your prediction in the type column\n",
    "  * Save your final Dataframe into a csv file\n",
    "    * You can google pandas.DataFrame.to_csv and use that function\n",
    "  * Make sure your submission file looks the same as this sample file: https://drive.google.com/file/d/16sjX5ofIbmqU3FbK5cRiVpwRwLppMQxg/view?usp=sharing\n",
    "  * Rename your submission file into \"yourname_submission.csv\"\n",
    "6. Upload both your Notebook and submission file onto the Google Classroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y36MyoZho5Wp"
   },
   "outputs": [],
   "source": [
    "# Final submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsCU7Z7WtHgX"
   },
   "source": [
    "# Congratulations on finishing your first big project!!!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "jinxiao Yang - PH Electives G10 2022 Spring Project Milestone #3 - Student Version.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
