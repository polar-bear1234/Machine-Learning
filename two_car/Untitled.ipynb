{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae6b14-2f9d-4658-8ce1-28035b795a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为天池学习赛\n",
    "#导入需要使用的库\n",
    "import warnings\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import LinearSVC\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "path = os.path.abspath(os.path.dirname(os.getcwd()) + os.path.sep + \".\")\n",
    "input_path = path + '\\\\pythonProject\\\\data\\\\'\n",
    "Train_data = pd.read_csv(input_path+'car_train_0110.csv', sep=' ')\n",
    "Test_data = pd.read_csv(input_path+'car_testA_0110.csv', sep=' ')\n",
    "# Test_data = pd.DataFrame(Test_data)\n",
    "# Train_data = pd.DataFrame(Train_data)\n",
    "# print(Train_data)\n",
    "# print(Train_data.head())\n",
    "# print(Test_data.head())\n",
    "#\n",
    "#\n",
    "# \"\"\"\n",
    "# —————————————————————————————————————————————以下为树模型的数据处理—————————————————————————————————————————————\n",
    "# \"\"\"\n",
    "# \"\"\"\n",
    "# 一、预测值处理，处理目标值长尾分布的问题\n",
    "# \"\"\"\n",
    "Train_data['price'] = np.log1p(Train_data['price'])\n",
    "#\n",
    "# # 合并方便后面的操作\n",
    "df = pd.concat([Train_data, Test_data], ignore_index=True)\n",
    "#\n",
    "# \"\"\"\n",
    "# 二、数据简单预处理，分三步进行\n",
    "# \"\"\"\n",
    "# ## 1、第一步处理无用值和基本无变化的值\n",
    "# #SaleID肯定没用，但是我们可以用来统计别的特征的group数量\n",
    "# #name一般没什么好挖掘的，不过同名的好像不少，可以挖掘一下\n",
    "# df['name_count'] = df.groupby(['name'])['SaleID'].transform('count')\n",
    "# del df['name']\n",
    "#\n",
    "#\n",
    "# #seller有一个特殊值，训练集特有测试集没有，把它删除掉\n",
    "# df.drop(df[df['seller'] == 1].index, inplace=True)\n",
    "# del df['offerType']\n",
    "# del df['seller']\n",
    "#\n",
    "#\n",
    "# ## 2、第二步处理缺失值\n",
    "# # 以下特征全部填充众数\n",
    "# df['fuelType'] = df['fuelType'].fillna(0)\n",
    "# df['gearbox'] = df['gearbox'].fillna(0)\n",
    "# df['bodyType'] = df['bodyType'].fillna(0)\n",
    "# df['model'] = df['model'].fillna(0)\n",
    "#\n",
    "#\n",
    "#\n",
    "# ## 3、第三步处理异常值\n",
    "#\n",
    "# # 异常值就目前初步判断，只有notRepairedDamage的值有问题，还有题目规定了范围的power。处理一下\n",
    "# df['power'] = df['power'].map(lambda x: 600 if x>600 else x)\n",
    "# df['notRepairedDamage'] = df['notRepairedDamage'].astype('str').apply(lambda x: x if x != '-' else None).astype('float32')\n",
    "#\n",
    "# \"\"\"\n",
    "# 三、以上为数据简单预处理，以下为特征工程（特征工程搞起来，分三大块整理一下）\n",
    "# \"\"\"\n",
    "# ## 1、时间，地区啥的\n",
    "#\n",
    "# #时间\n",
    "# from datetime import datetime\n",
    "# def date_process(x):\n",
    "#     year = int(str(x)[:4])\n",
    "#     month = int(str(x)[4:6])\n",
    "#     day = int(str(x)[6:8])\n",
    "#\n",
    "#     if month < 1:\n",
    "#         month = 1\n",
    "#\n",
    "#     date = datetime(year, month, day)\n",
    "#     date = pd.to_datetime(date)\n",
    "#     return date\n",
    "#\n",
    "# df['regDate'] = df['regDate'].apply(date_process)\n",
    "# df['creatDate'] = df['creatDate'].apply(date_process)\n",
    "# df['regDate_year'] = df['regDate'].dt.year\n",
    "# df['regDate_month'] = df['regDate'].dt.month\n",
    "# df['regDate_day'] = df['regDate'].dt.day\n",
    "# df['creatDate_year'] = df['creatDate'].dt.year\n",
    "# df['creatDate_month'] = df['creatDate'].dt.month\n",
    "# df['creatDate_day'] = df['creatDate'].dt.day\n",
    "# df['car_age_day'] = (df['creatDate'] - df['regDate']).dt.days\n",
    "# df['car_age_year'] = round(df['car_age_day'] / 365, 1)\n",
    "#\n",
    "\n",
    "# #地区\n",
    "# df['regionCode_count'] = df.groupby(['regionCode'])['SaleID'].transform('count')\n",
    "# df['city'] = df['regionCode'].apply(lambda x : str(x)[:2])\n",
    "#\n",
    "# #\n",
    "# # ## 2、分类特征\n",
    "# # # 对可分类的连续特征进行分桶，kilometer是已经分桶了\n",
    "# bin = [i*10 for i in range(31)]\n",
    "# df['power_bin'] = pd.cut(df['power'], bin, labels=False)\n",
    "# tong = df[['power_bin', 'power']].head()\n",
    "#\n",
    "#\n",
    "# bin = [i*10 for i in range(24)]\n",
    "# df['model_bin'] = pd.cut(df['model'], bin, labels=False)\n",
    "# tong = df[['model_bin', 'model']].head()\n",
    "# #\n",
    "# # 将稍微取值多一点的分类特征与price进行特征组合，做了非常多组，但是在最终使用的时候，每组分开测试，挑选真正work的特征\n",
    "# Train_gb = Train_data.groupby(\"regionCode\")\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#\n",
    "#     kind_data = kind_data[kind_data['price'] > 0]\n",
    "#     info['regionCode_amount'] = len(kind_data)\n",
    "#     info['regionCode_price_max'] = kind_data.price.max()\n",
    "#     info['regionCode_price_median'] = kind_data.price.median()\n",
    "#     info['regionCode_price_min'] = kind_data.price.min()\n",
    "#     info['regionCode_price_sum'] = kind_data.price.sum()\n",
    "#     info['regionCode_price_std'] = kind_data.price.std()\n",
    "#     info['regionCode_price_mean'] = kind_data.price.mean()\n",
    "#     info['regionCode_price_skew'] = kind_data.price.skew()\n",
    "#     info['regionCode_price_kurt'] = kind_data.price.kurt()\n",
    "#     info['regionCode_mad'] = kind_data.price.mad()\n",
    "#\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"regionCode\"})\n",
    "# df = df.merge(brand_fe, how='left', on='regionCode')\n",
    "#\n",
    "# Train_gb = Train_data.groupby(\"brand\")\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data['price'] > 0]\n",
    "#     info['brand_amount'] = len(kind_data)\n",
    "#     info['brand_price_max'] = kind_data.price.max()\n",
    "#     info['brand_price_median'] = kind_data.price.median()\n",
    "#     info['brand_price_min'] = kind_data.price.min()\n",
    "#     info['brand_price_sum'] = kind_data.price.sum()\n",
    "#     info['brand_price_std'] = kind_data.price.std()\n",
    "#     info['brand_price_mean'] = kind_data.price.mean()\n",
    "#     info['brand_price_skew'] = kind_data.price.skew()\n",
    "#     info['brand_price_kurt'] = kind_data.price.kurt()\n",
    "#     info['brand_price_mad'] = kind_data.price.mad()\n",
    "#\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"brand\"})\n",
    "# df = df.merge(brand_fe, how='left', on='brand')\n",
    "#\n",
    "# Train_gb = Train_data.groupby(\"model\")\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data['price'] > 0]\n",
    "#     info['model_amount'] = len(kind_data)\n",
    "#     info['model_price_max'] = kind_data.price.max()\n",
    "#     info['model_price_median'] = kind_data.price.median()\n",
    "#     info['model_price_min'] = kind_data.price.min()\n",
    "#     info['model_price_sum'] = kind_data.price.sum()\n",
    "#     info['model_price_std'] = kind_data.price.std()\n",
    "#     info['model_price_mean'] = kind_data.price.mean()\n",
    "#     info['model_price_skew'] = kind_data.price.skew()\n",
    "#     info['model_price_kurt'] = kind_data.price.kurt()\n",
    "#     info['model_price_mad'] = kind_data.price.mad()\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"model\"})\n",
    "# df = df.merge(brand_fe, how='left', on='model')\n",
    "#\n",
    "# Train_gb = Train_data.groupby(\"kilometer\")\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data['price'] > 0]\n",
    "#     info['kilometer_amount'] = len(kind_data)\n",
    "#     info['kilometer_price_max'] = kind_data.price.max()\n",
    "#     info['kilometer_price_median'] = kind_data.price.median()\n",
    "#     info['kilometer_price_min'] = kind_data.price.min()\n",
    "#     info['kilometer_price_sum'] = kind_data.price.sum()\n",
    "#     info['kilometer_price_std'] = kind_data.price.std()\n",
    "#     info['kilometer_price_mean'] = kind_data.price.mean()\n",
    "#     info['kilometer_price_skew'] = kind_data.price.skew()\n",
    "#     info['kilometer_price_kurt'] = kind_data.price.kurt()\n",
    "#     info['kilometer_price_mad'] = kind_data.price.mad()\n",
    "#\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"kilometer\"})\n",
    "# df = df.merge(brand_fe, how='left', on='kilometer')\n",
    "#\n",
    "# Train_gb = Train_data.groupby(\"bodyType\")\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data['price'] > 0]\n",
    "#     info['bodyType_amount'] = len(kind_data)\n",
    "#     info['bodyType_price_max'] = kind_data.price.max()\n",
    "#     info['bodyType_price_median'] = kind_data.price.median()\n",
    "#     info['bodyType_price_min'] = kind_data.price.min()\n",
    "#     info['bodyType_price_sum'] = kind_data.price.sum()\n",
    "#     info['bodyType_price_std'] = kind_data.price.std()\n",
    "#     info['bodyType_price_mean'] = kind_data.price.mean()\n",
    "#     info['bodyType_price_skew'] = kind_data.price.skew()\n",
    "#     info['bodyType_price_kurt'] = kind_data.price.kurt()\n",
    "#     info['bodyType_price_mad'] = kind_data.price.mad()\n",
    "#\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"bodyType\"})\n",
    "# df = df.merge(brand_fe, how='left', on='bodyType')\n",
    "#\n",
    "#\n",
    "# Train_gb = Train_data.groupby(\"fuelType\")\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data['price'] > 0]\n",
    "#     info['fuelType_amount'] = len(kind_data)\n",
    "#     info['fuelType_price_max'] = kind_data.price.max()\n",
    "#     info['fuelType_price_median'] = kind_data.price.median()\n",
    "#     info['fuelType_price_min'] = kind_data.price.min()\n",
    "#     info['fuelType_price_sum'] = kind_data.price.sum()\n",
    "#     info['fuelType_price_std'] = kind_data.price.std()\n",
    "#     info['fuelType_price_mean'] = kind_data.price.mean()\n",
    "#     info['fuelType_price_skew'] = kind_data.price.skew()\n",
    "#     info['fuelType_price_kurt'] = kind_data.price.kurt()\n",
    "#     info['fuelType_price_mad'] = kind_data.price.mad()\n",
    "#\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": \"fuelType\"})\n",
    "# df = df.merge(brand_fe, how='left', on='fuelType')\n",
    "\n",
    "\n",
    "#\n",
    "#\n",
    "# # 测试分类特征与price时，发现有点效果，立马对model进行处理\n",
    "kk = \"regionCode\"\n",
    "# Train_gb = df.groupby(kk)\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data['car_age_day'] > 0]\n",
    "#     info[kk+'_days_max'] = kind_data.car_age_day.max()\n",
    "#     info[kk+'_days_min'] = kind_data.car_age_day.min()\n",
    "#     info[kk+'_days_std'] = kind_data.car_age_day.std()\n",
    "#     info[kk+'_days_mean'] = kind_data.car_age_day.mean()\n",
    "#     info[kk+'_days_median'] = kind_data.car_age_day.median()\n",
    "#     info[kk+'_days_sum'] = kind_data.car_age_day.sum()\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": kk})\n",
    "# df = df.merge(brand_fe, how='left', on=kk)\n",
    "\n",
    "# Train_gb = df.groupby(kk)\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data['power'] > 0]\n",
    "#     info[kk+'_power_max'] = kind_data.power.max()\n",
    "#     info[kk+'_power_min'] = kind_data.power.min()\n",
    "#     info[kk+'_power_std'] = kind_data.power.std()\n",
    "#     info[kk+'_power_mean'] = kind_data.power.mean()\n",
    "#     info[kk+'_power_median'] = kind_data.power.median()\n",
    "#     info[kk+'_power_sum'] = kind_data.power.sum()\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": kk})\n",
    "# df = df.merge(brand_fe, how='left', on=kk)\n",
    "#\n",
    "# ## 3、连续数值特征\n",
    "# # 都是匿名特征 比较训练集和测试集分布 分析完 基本没什么问题 先暂且全部保留咯\n",
    "# # 后期也许得对相似度较大的进行剔除处理\n",
    "# # 对简易lgb模型输出的特征重要度较高的几个连续数值特征对price进行刻画\n",
    "#\n",
    "# dd = 'v_3'\n",
    "# Train_gb = df.groupby(kk)\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data[dd] > -10000000]\n",
    "#     info[kk+'_'+dd+'_max'] = kind_data.v_3.max()\n",
    "#     info[kk+'_'+dd+'_min'] = kind_data.v_3.min()\n",
    "#     info[kk+'_'+dd+'_std'] = kind_data.v_3.std()\n",
    "#     info[kk+'_'+dd+'_mean'] = kind_data.v_3.mean()\n",
    "#     info[kk+'_'+dd+'_median'] = kind_data.v_3.median()\n",
    "#     info[kk+'_'+dd+'_sum'] = kind_data.v_3.sum()\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": kk})\n",
    "# df = df.merge(brand_fe, how='left', on=kk)\n",
    "\n",
    "\n",
    "\n",
    "# dd = 'v_8'\n",
    "# Train_gb = df.groupby(kk)\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data[dd]> -10000000]\n",
    "#     info[kk+'_'+dd+'_max'] = kind_data.v_8.max()\n",
    "#     info[kk+'_'+dd+'_min'] = kind_data.v_8.min()\n",
    "#     info[kk+'_'+dd+'_std'] = kind_data.v_8.std()\n",
    "#     info[kk+'_'+dd+'_mean'] = kind_data.v_8.mean()\n",
    "#     info[kk+'_'+dd+'_median'] = kind_data.v_8.median()\n",
    "#     info[kk+'_'+dd+'_sum'] = kind_data.v_8.sum()\n",
    "#     all_info[kind] = info\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": kk})\n",
    "# df = df.merge(brand_fe, how='left', on=kk)\n",
    "#\n",
    "# dd = 'v_7'\n",
    "# Train_gb = df.groupby(kk)\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data[dd]> -10000000]\n",
    "#     info[kk+'_'+dd+'_max'] = kind_data.v_7.max()\n",
    "#     info[kk+'_'+dd+'_min'] = kind_data.v_7.min()\n",
    "#     info[kk+'_'+dd+'_std'] = kind_data.v_7.std()\n",
    "#     info[kk+'_'+dd+'_mean'] = kind_data.v_7.mean()\n",
    "#     info[kk+'_'+dd+'_median'] = kind_data.v_7.median()\n",
    "#     info[kk+'_'+dd+'_sum'] = kind_data.v_7.sum()\n",
    "#     all_info[kind] = info\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": kk})\n",
    "# df = df.merge(brand_fe, how='left', on=kk)\n",
    "#\n",
    "# dd = 'v_14'\n",
    "# Train_gb = df.groupby(kk)\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data[dd]> -10000000]\n",
    "#     info[kk+'_'+dd+'_max'] = kind_data.v_14.max()\n",
    "#     info[kk+'_'+dd+'_min'] = kind_data.v_14.min()\n",
    "#     info[kk+'_'+dd+'_std'] = kind_data.v_14.std()\n",
    "#     info[kk+'_'+dd+'_mean'] = kind_data.v_14.mean()\n",
    "#     info[kk+'_'+dd+'_median'] = kind_data.v_14.median()\n",
    "#     info[kk+'_'+dd+'_sum'] = kind_data.v_14.sum()\n",
    "#     all_info[kind] = info\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": kk})\n",
    "# df = df.merge(brand_fe, how='left', on=kk)\n",
    "#\n",
    "# dd = 'v_22'\n",
    "# Train_gb = df.groupby(kk)\n",
    "# all_info = {}\n",
    "# for kind, kind_data in Train_gb:\n",
    "#     info = {}\n",
    "#     kind_data = kind_data[kind_data[dd]> -10000000]\n",
    "#     info[kk+'_'+dd+'_max'] = kind_data.v_22.max()\n",
    "#     info[kk+'_'+dd+'_min'] = kind_data.v_22.min()\n",
    "#     info[kk+'_'+dd+'_std'] = kind_data.v_22.std()\n",
    "#     info[kk+'_'+dd+'_mean'] = kind_data.v_22.mean()\n",
    "#     info[kk+'_'+dd+'_median'] = kind_data.v_22.median()\n",
    "#     info[kk+'_'+dd+'_sum'] = kind_data.v_22.sum()\n",
    "#     all_info[kind] = info\n",
    "#     all_info[kind] = info\n",
    "# brand_fe = pd.DataFrame(all_info).T.reset_index().rename(columns={\"index\": kk})\n",
    "# df = df.merge(brand_fe, how='left', on=kk)\n",
    "#\n",
    "# for i in range(24):\n",
    "#     for j in range(24):\n",
    "#         df['new'+str(i)+'*'+str(j)] = df['v_'+str(i)]*df['v_'+str(j)]\n",
    "#\n",
    "# for i in range(24):\n",
    "#     for j in range(24):\n",
    "#         df['new'+str(i)+'+'+str(j)] = df['v_'+str(i)]+df['v_'+str(j)]\n",
    "#\n",
    "# sanfea = ['v16', 'v18', 'v3', 'kilometer_price_min', 'bodyType_price_min', 'fuelType_price_min', 'regDate', 'price',\n",
    "#           'SaleID']\n",
    "# # feature = df.select_dtypes(exclude='object').columns\n",
    "# feature = df.columns\n",
    "# print(feature)\n",
    "# feature = [col for col in feature if col not in sanfea]\n",
    "# print(feature)\n",
    "# data_iris = df[feature]\n",
    "# target_iris = Train_data['price']\n",
    "# print('----------------------')\n",
    "# data_iris = np.array(data_iris)\n",
    "# target_iris = np.array(target_iris)\n",
    "# print(np.isnan(data_iris).any())\n",
    "# np.isnan(data_iris).to_csv('nono.csv', index=False)\n",
    "# print(np.isnan(data_iris).any())\n",
    "# print(data_iris.isnull().any())\n",
    "# print('---------------------')\n",
    "# model_lsvc = LinearSVC(penalty='l1', C=0.01, dual=False)\n",
    "# data_iris = np.array(data_iris)\n",
    "# target_iris = np.array(target_iris)\n",
    "# model_lsvc.fit(data_iris, target_iris)\n",
    "# print('-------------------------')\n",
    "# print(model_lsvc.coef_)\n",
    "# model_sfm = SelectFromModel(model_lsvc, prefit=True)\n",
    "# print('------------------------')\n",
    "# print(model_sfm.transform(data_iris).shape)\n",
    "# corr = df.corr()\n",
    "# corr = corr['price'].sort_values(ascending=False)\n",
    "# print(corr)\n",
    "# ls = list(corr.index)\n",
    "# print(ls)\n",
    "# corr.to_csv('xiangguanxing.csv')\n",
    "#\n",
    "# \"\"\"\n",
    "# 四、补充的特征工程\n",
    "# \"\"\"\n",
    "# ## 主要是对匿名特征和几个重要度较高的分类特征进行特征交叉\n",
    "# #第一批特征工程\n",
    "# for i in range(15):\n",
    "#     for j in range(15):\n",
    "#         df['new'+str(i)+'*'+str(j)]=df['v_'+str(i)]*df['v_'+str(j)]\n",
    "#\n",
    "#\n",
    "# #第二批特征工程\n",
    "# for i in range(15):\n",
    "#     for j in range(15):\n",
    "#         df['new'+str(i)+'+'+str(j)]=df['v_'+str(i)]+df['v_'+str(j)]\n",
    "#\n",
    "# # 第三批特征工程\n",
    "# for i in range(15):\n",
    "#     df['new' + str(i) + '*power'] = df['v_' + str(i)] * df['power']\n",
    "#\n",
    "# for i in range(15):\n",
    "#     df['new' + str(i) + '*day'] = df['v_' + str(i)] * df['car_age_day']\n",
    "#\n",
    "# for i in range(15):\n",
    "#     df['new' + str(i) + '*year'] = df['v_' + str(i)] * df['car_age_year']\n",
    "#\n",
    "#\n",
    "# #第四批特征工程\n",
    "# for i in range(15):\n",
    "#     for j in range(15):\n",
    "#         df['new'+str(i)+'-'+str(j)]=df['v_'+str(i)]-df['v_'+str(j)]\n",
    "#\n",
    "#\n",
    "# \"\"\"\n",
    "# 五、筛选特征\n",
    "# \"\"\"                                                      \n",
    "# numerical_cols = df.select_dtypes(exclude='object').columns\n",
    "#\n",
    "# list_tree = [ 'model_power_sum','price','SaleID',\n",
    "#  'model_power_std', 'model_power_median', 'model_power_max',\n",
    "#  'brand_price_max', 'brand_price_median',\n",
    "#  'brand_price_sum', 'brand_price_std',\n",
    "#  'model_days_sum',\n",
    "#  'model_days_std', 'model_days_median', 'model_days_max', 'model_bin', 'model_amount',\n",
    "#  'model_price_max', 'model_price_median',\n",
    "#  'model_price_min', 'model_price_sum', 'model_price_std',\n",
    "#  'model_price_mean', 'bodyType', 'model', 'brand', 'fuelType', 'gearbox', 'power', 'kilometer',\n",
    "#  'notRepairedDamage', 'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6', 'v_7', 'v_8', 'v_9', 'v_10',\n",
    "#  'v_11', 'v_12', 'v_13', 'v_14', 'name_count', 'regDate_year', 'car_age_day', 'car_age_year',\n",
    "#  'power_bin','fuelType', 'gearbox', 'kilometer', 'notRepairedDamage',  'name_count', 'car_age_day', 'new3*3', 'new12*14', 'new2*14','new14*14']\n",
    "#\n",
    "# for i in range(15):\n",
    "#     for j in range(15):\n",
    "#         list_tree.append('new'+str(i)+'+'+str(j))\n",
    "#\n",
    "# feature_cols = [col for col in numerical_cols if\n",
    "#              col  in\n",
    "#              list_tree]\n",
    "#\n",
    "# feature_cols = [col for col in feature_cols if\n",
    "#              col  not in\n",
    "#              ['new14+6', 'new13+6', 'new0+12', 'new9+11', 'v_3', 'new11+10', 'new10+14', 'new12+4', 'new3+4', 'new11+11', 'new13+3', 'new8+1', 'new1+7', 'new11+14', 'new8+13', 'v_8', 'v_0', 'new3+5', 'new2+9', 'new9+2', 'new0+11', 'new13+7', 'new8+11', 'new5+12', 'new10+10', 'new13+8', 'new11+13', 'new7+9', 'v_1', 'new7+4', 'new13+4', 'v_7', 'new5+6', 'new7+3', 'new9+10', 'new11+12', 'new0+5', 'new4+13', 'new8+0', 'new0+7', 'new12+8', 'new10+8', 'new13+14', 'new5+7', 'new2+7', 'v_4', 'v_10', 'new4+8', 'new8+14', 'new5+9', 'new9+13', 'new2+12', 'new5+8', 'new3+12', 'new0+10', 'new9+0', 'new1+11', 'new8+4', 'new11+8', 'new1+1', 'new10+5', 'new8+2', 'new6+1', 'new2+1', 'new1+12', 'new2+5', 'new0+14', 'new4+7', 'new14+9', 'new0+2', 'new4+1', 'new7+11', 'new13+10', 'new6+3', 'new1+10', 'v_9', 'new3+6', 'new12+1', 'new9+3', 'new4+5', 'new12+9', 'new3+8', 'new0+8', 'new1+8', 'new1+6', 'new10+9', 'new5+4', 'new13+1', 'new3+7', 'new6+4', 'new6+7', 'new13+0', 'new1+14', 'new3+11', 'new6+8', 'new0+9', 'new2+14', 'new6+2', 'new12+12', 'new7+12', 'new12+6', 'new12+14', 'new4+10', 'new2+4', 'new6+0', 'new3+9', 'new2+8', 'new6+11', 'new3+10', 'new7+0', 'v_11', 'new1+3', 'new8+3', 'new12+13', 'new1+9', 'new10+13', 'new5+10', 'new2+2', 'new6+9', 'new7+10', 'new0+0', 'new11+7', 'new2+13', 'new11+1', 'new5+11', 'new4+6', 'new12+2', 'new4+4', 'new6+14', 'new0+1', 'new4+14', 'v_5', 'new4+11', 'v_6', 'new0+4', 'new1+5', 'new3+14', 'new2+10', 'new9+4', 'new2+6', 'new14+14', 'new11+6', 'new9+1', 'new3+13', 'new13+13', 'new10+6', 'new2+3', 'new2+11', 'new1+4', 'v_2', 'new5+13', 'new4+2', 'new0+6', 'new7+13', 'new8+9', 'new9+12', 'new0+13', 'new10+12', 'new5+14', 'new6+10', 'new10+7', 'v_13', 'new5+2', 'new6+13', 'new9+14', 'new13+9', 'new14+7', 'new8+12', 'new3+3', 'new6+12', 'v_12', 'new14+4', 'new11+9', 'new12+7', 'new4+9', 'new4+12', 'new1+13', 'new0+3', 'new8+10', 'new13+11', 'new7+8', 'new7+14', 'v_14', 'new10+11', 'new14+8', 'new1+2']]\n",
    "#\n",
    "# df = df[feature_cols]\n",
    "#\n",
    "#\n",
    "# \"\"\"\n",
    "# 六、导出数据\n",
    "# \"\"\"\n",
    "# ## 切割数据,导出数据,作为树模型的训练数据\n",
    "#\n",
    "# output_path = path + '\\\\pythonProject\\\\data\\\\'\n",
    "# tree_data = df\n",
    "# print(tree_data.shape)\n",
    "# train_num = df.shape[0]-50000\n",
    "# tree_data[0:int(train_num)].to_csv(output_path+'train_tree.csv', index=0,sep=' ')\n",
    "# tree_data[train_num:train_num+50000].to_csv(output_path+'text_tree.csv', index=0,sep=' ')\n",
    "#\n",
    "# print('树模型数据已经准备完毕~~~~~~~')\n",
    "#\n",
    "#\n",
    "#\n",
    "# \"\"\"\n",
    "# —————————————————————————————————————————————以下为神经网络的数据处理—————————————————————————————————————————————\n",
    "# \"\"\"\n",
    "# input_path = path + '\\\\pythonProject\\\\data\\\\'\n",
    "# Train_data = pd.read_csv(input_path+'car_train_0110.csv', sep=' ')\n",
    "# Test_data = pd.read_csv(input_path+'car_testA_0110.csv', sep=' ')\n",
    "#\n",
    "# # 合并方便后面的操作\n",
    "# df = pd.concat([Train_data, Test_data], ignore_index=True)\n",
    "#\n",
    "#\n",
    "# #选择需要使用的特征标签，由于nn会生成大量的特征，我们只需要保留原始特征和刻画几个明显特征即可\n",
    "# feature = ['model','brand','bodyType','fuelType','kilometer','notRepairedDamage','power','regDate_month','creatDate_year','creatDate_month'\n",
    "#     ,'v_0', 'v_1', 'v_2', 'v_3', 'v_4', 'v_5', 'v_6',\n",
    "#        'v_7', 'v_8', 'v_9', 'v_10', 'v_11', 'v_12', 'v_13', 'v_14','car_age_day','car_age_year','regDate_year','name_count']\n",
    "#\n",
    "#\n",
    "# #处理异常数据\n",
    "# # df.drop(df[df['seller'] == 1].index, inplace=True)\n",
    "# #记录一下df的price\n",
    "# df_copy = df\n",
    "#\n",
    "# df['power'][df['power']>600]=600\n",
    "#\n",
    "# #notRepairedDamage的值是0和1，然后为-的值设置为0.5，在将它进行标签转换，0->1;0.5->2;1->3;这样符合神经网络的特征提取，不确定值位于两个确定值的中间～\n",
    "# df.replace(to_replace = '-', value = 0.5, inplace = True)\n",
    "# le = LabelEncoder()\n",
    "# df['notRepairedDamage'] = le.fit_transform(df['notRepairedDamage'].astype(str))\n",
    "#\n",
    "# #日期处理\n",
    "# from datetime import datetime\n",
    "# def date_process(x):\n",
    "#     year = int(str(x)[:4])\n",
    "#     month = int(str(x)[4:6])\n",
    "#     day = int(str(x)[6:8])\n",
    "#\n",
    "#     if month < 1:\n",
    "#         month = 1\n",
    "#\n",
    "#     date = datetime(year, month, day)\n",
    "#     return date\n",
    "#\n",
    "# df['regDates'] = df['regDate'].apply(date_process)\n",
    "# df['creatDates'] = df['creatDate'].apply(date_process)\n",
    "# df['regDate_year'] = df['regDates'].dt.year\n",
    "# df['regDate_month'] = df['regDates'].dt.month\n",
    "# df['regDate_day'] = df['regDates'].dt.day\n",
    "# df['creatDate_year'] = df['creatDates'].dt.year\n",
    "# df['creatDate_month'] = df['creatDates'].dt.month\n",
    "# df['creatDate_day'] = df['creatDates'].dt.day\n",
    "# df['car_age_day'] = (df['creatDates'] - df['regDates']).dt.days\n",
    "# df['car_age_year'] = round(df['car_age_day'] / 365, 1)\n",
    "#\n",
    "# #对name进行挖掘\n",
    "# df['name_count'] = df.groupby(['name'])['SaleID'].transform('count')\n",
    "#\n",
    "# #填充众数\n",
    "# df.fillna(df.median(),inplace= True)\n",
    "#\n",
    "#\n",
    "#\n",
    "# #特征归一化\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(df[feature].values)\n",
    "# df= scaler.transform(df[feature].values)\n",
    "#\n",
    "#\n",
    "# ## 切割数据,导出数据,作为神经网络的训练数据\n",
    "# output_path = path + '\\\\pythonProject\\\\data\\\\'\n",
    "# nn_data = pd.DataFrame(df, columns=feature)\n",
    "# nn_data['price']=np.array(df_copy['price'])\n",
    "# nn_data['SaleID']=np.array(df_copy['SaleID'])\n",
    "# print(nn_data.shape)\n",
    "# train_num = df.shape[0]-50000\n",
    "# nn_data[0:int(train_num)].to_csv(output_path+'train_nn.csv', index=0, sep=' ')\n",
    "# nn_data[train_num:train_num+50000].to_csv(output_path+'test_nn.csv', index=0, sep=' ')\n",
    "#\n",
    "# print('NN模型数据已经准备完毕~~~~~~~')\n",
    "\n",
    "\n",
    "\n",
    "## 读取树模型数据\n",
    "# path = os.path.abspath(os.path.dirname(os.getcwd()) + os.path.sep + \".\")\n",
    "# tree_data_path = path + '\\\\pythonProject\\\\data\\\\'\n",
    "# Train_data = pd.read_csv(tree_data_path+'train_tree.csv', sep=' ')\n",
    "# TestA_data = pd.read_csv(tree_data_path+'text_tree.csv', sep=' ')\n",
    "\n",
    "# numerical_cols = Train_data.columns\n",
    "# feature_cols = [col for col in numerical_cols if col not in ['price', 'SaleID']]\n",
    "# print(feature_cols)\n",
    "faea = []\n",
    "for i in range(24):\n",
    "    i = str(i)\n",
    "    a = 'v_' + i\n",
    "    faea.append(a)\n",
    "# X_data = Train_data[faea]\n",
    "nuum = Train_data.columns\n",
    "faea = [col for col in nuum if col in faea]\n",
    "print(faea)\n",
    "# X_data = Train_data[faea]\n",
    "# X_test = Test_data[faea]\n",
    "# print(X_data.info())\n",
    "# X_data = np.array(X_data)\n",
    "# X_test = np.array(X_test)\n",
    "# Y_data = np.array(Train_data['price'])\n",
    "# Y_data = Train_data['price']\n",
    "# print(X_data.columns)\n",
    "# print(X_data.shape)\n",
    "# print(X_test.shape)\n",
    "# print(Y_data.shape)\n",
    "# print(X_data)\n",
    "# X_data = Train_data\n",
    "# X_test = Test_data\n",
    "# Y_data = Train_data['price']\n",
    "# print(X_data.shape)\n",
    "# print(Y_data.shape)\n",
    "# print(X_test.shape)\n",
    "# X_data = np.array(X_data)\n",
    "# X_test = np.array(X_test)\n",
    "# Y_data = np.array(Y_data)\n",
    "## 提前特征列，标签列构造训练样本和测试样本\n",
    "X_data = Train_data[faea]\n",
    "X_test = Test_data[faea]\n",
    "# print(X_data.shape)\n",
    "# print(X_test.shape)\n",
    "Y_data = Train_data['price']\n",
    "# X_data = np.array(X_data)\n",
    "# X_test = np.array(X_test)\n",
    "# Y_data = np.array(Train_data['price'])\n",
    "\n",
    "test_gx = X_test\n",
    "predictions_lgb = np.zeros((len(test_gx)))\n",
    "# train_data = X_data\n",
    "# train_label = Y_data\n",
    "# X_data = X_data.drop(['price'], axis=1)\n",
    "\n",
    "def select_by_lgb(train_data, train_label, random_state=2020, n_splits=5, metric='mse', num_round=10000,\n",
    "                  early_stopping_rounds=100):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances['feature'] = train_data.columns\n",
    "    fold = 0\n",
    "    for train_idx, val_idx in kfold.split(train_data):\n",
    "        random_state += 1\n",
    "        train_x = train_data.loc[train_idx]\n",
    "        train_y = train_label.loc[train_idx]\n",
    "        test_x = train_data.loc[val_idx]\n",
    "        test_y = train_label.loc[val_idx]\n",
    "        clf = lgb\n",
    "        train_matrix = clf.Dataset(train_x, label=train_y)\n",
    "        test_matrix = clf.Dataset(test_x, label=test_y)\n",
    "        params = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'num_leaves': 31,\n",
    "            'max_bin': 50,\n",
    "            'max_depth': 6,\n",
    "            \"learning_rate\": 0.02,\n",
    "            \"colsample_bytree\": 0.8,  # 每次迭代中随机选择特征的比例\n",
    "            \"bagging_fraction\": 0.8,  # 每次迭代时用的数据比例\n",
    "            'min_child_samples': 25,\n",
    "            'n_jobs': -1,\n",
    "            'silent': True,  # 信息输出设置成1则没有信息输出\n",
    "            'seed': 1000,\n",
    "        }  # 设置出参数\n",
    "        num_round = 200\n",
    "        model = lgb.train(params, train_matrix, num_boost_round=400, valid_sets=test_matrix, verbose_eval=500,\n",
    "                          early_stopping_rounds=200)\n",
    "        feature_importances['fold_{}'.format(fold + 1)] = model.feature_importance()\n",
    "        predictions_lgb[:] += model.predict(test_gx, num_iteration=model.best_iteration) / n_splits\n",
    "        fold += 1\n",
    "    feature_importances['averge'] = feature_importances[['fold_{}'.format(i) for i in range(1, n_splits + 1)]].mean(\n",
    "        axis=1)\n",
    "    return feature_importances, predictions_lgb\n",
    "\n",
    "\n",
    "feature_importances, predictions_lgb = select_by_lgb(X_data, Y_data)\n",
    "feature_importances['averge'] = feature_importances[['fold_{}'.format(i) for i in range(1, 6)]].mean(axis=1)\n",
    "print('-------------------------')\n",
    "print(feature_importances)\n",
    "\"\"\"\n",
    "lightgbm\n",
    "\"\"\"\n",
    "# 自定义损失函数\n",
    "# print(X_data)\n",
    "# print(X_data.iloc[[4]])\n",
    "# print(X_data.iloc[[5]])\n",
    "#\n",
    "# def myFeval(preds, xgbtrain):\n",
    "#     label = xgbtrain.get_label()\n",
    "#     score = mean_absolute_error(np.expm1(label), np.expm1(preds))\n",
    "#     return 'myFeval', score, False\n",
    "#\n",
    "# param = {'boosting_type': 'gbdt',\n",
    "#          'num_leaves': 31,\n",
    "#          'max_depth': -1,\n",
    "#          \"lambda_l2\": 2,  # 防止过拟合\n",
    "#          'min_data_in_leaf': 20,  # 防止过拟合，好像都不用怎么调\n",
    "#          'objective': 'regression_l1',\n",
    "#          'learning_rate': 0.01,\n",
    "#          \"min_child_samples\": 20,\n",
    "#          \"feature_fraction\": 0.8,\n",
    "#          \"bagging_freq\": 1,\n",
    "#          \"bagging_fraction\": 0.8,\n",
    "#          \"bagging_seed\": 11,\n",
    "#          \"metric\": 'mae',\n",
    "#          }\n",
    "#\n",
    "# folds = KFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "# oof_lgb = np.zeros(len(X_data))\n",
    "# predictions_lgb = np.zeros(len(X_test))\n",
    "# predictions_train_lgb = np.zeros(len(X_data))\n",
    "#\n",
    "# print(X_data.info())\n",
    "# print('新的qqqqq')\n",
    "# for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_data, Y_data)):\n",
    "# # for trn_idx, val_idx in folds.split(X_data):\n",
    "#\n",
    "#     print(trn_idx)\n",
    "#     print(val_idx)\n",
    "#     # print(\"fold n°{}\".format(fold_ + 1))\n",
    "#     print('------------------------')\n",
    "#     print(X_data.iloc[trn_idx])\n",
    "#     print('gogogogoogo')\n",
    "#     print(X_data.iloc[val_idx])\n",
    "#     print('---------------------')\n",
    "#     trn_data = lgb.Dataset(X_data.iloc[trn_idx], Y_data.iloc[trn_idx])\n",
    "#     val_data = lgb.Dataset(X_data.iloc[val_idx], Y_data.iloc[val_idx])\n",
    "    #\n",
    "    # num_round = 100000000\n",
    "    # clf = lgb.train(param, trn_data, num_round, valid_sets=[trn_data, val_data], verbose_eval=300,\n",
    "    #                 early_stopping_rounds=600, feval=myFeval)\n",
    "    # pickle\n",
    "    # with open('saved_model_lgb.pickle', 'wb') as f:\n",
    "    #     pickle.dump(lgb, f)\n",
    "    # joblic\n",
    "    # joblib.dump(lgb, 'saved_model/lgb.pkl')\n",
    "#     oof_lgb[val_idx] = clf.predict(X_data[val_idx], num_iteration=clf.best_iteration)\n",
    "#     print(oof_lgb)\n",
    "#     predictions_lgb += clf.predict(X_test, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "#     print(predictions_lgb)\n",
    "#     predictions_train_lgb += clf.predict(X_data, num_iteration=clf.best_iteration) / folds.n_splits\n",
    "# print(\"lightgbm score: {:<8.8f}\".format(mean_absolute_error(np.expm1(oof_lgb), np.expm1(Y_data))))\n",
    "# # pickle\n",
    "# with open('saved_model_lgb.pickle', 'wb') as f:\n",
    "#     pickle.dump(lgb, f)\n",
    "# f = open('saved_model/lgb.pickle', 'wb')\n",
    "# pickle.dump(lgb, f)\n",
    "# f.close()\n",
    "# joblic\n",
    "# joblib.dump(lgb, 'saved_model_lgb.pkl')\n",
    "\n",
    "# output_path = path + path + '\\\\pythonProject\\\\data\\\\'\n",
    "# # 测试集输出\n",
    "# predictions = predictions_lgb\n",
    "# predictions[predictions < 0] = 0\n",
    "# sub = pd.DataFrame()\n",
    "# sub['SaleID'] = TestA_data.SaleID\n",
    "# sub['price'] = predictions\n",
    "# sub.to_csv(output_path+'lgb_test.csv', index=False)\n",
    "#\n",
    "#\n",
    "# # 验证集输出\n",
    "# oof_lgb[oof_lgb < 0] = 0\n",
    "# sub = pd.DataFrame()\n",
    "# sub['SaleID'] = Train_data.SaleID\n",
    "# sub['price'] = oof_lgb\n",
    "# sub.to_csv(output_path+'lgb_train.csv', index=False)\n",
    "#\n",
    "#\n",
    "# \"\"\"\n",
    "# catboost\n",
    "# \"\"\"\n",
    "# kfolder = KFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "# oof_cb = np.zeros(len(X_data))\n",
    "# predictions_cb = np.zeros(len(X_test))\n",
    "# predictions_train_cb = np.zeros(len(X_data))\n",
    "# kfold = kfolder.split(X_data, Y_data)\n",
    "# fold_ = 0\n",
    "# for train_index, vali_index in kfold:\n",
    "#     fold_ = fold_ + 1\n",
    "#     print(\"fold n°{}\".format(fold_))\n",
    "#     k_x_train = X_data[train_index]\n",
    "#     k_y_train = Y_data[train_index]\n",
    "#     k_x_vali = X_data[vali_index]\n",
    "#     k_y_vali = Y_data[vali_index]\n",
    "#     cb_params = {\n",
    "#         'n_estimators': 100000000,\n",
    "#         'loss_function': 'MAE',\n",
    "#         'eval_metric': 'MAE',\n",
    "#         'learning_rate': 0.01,\n",
    "#         'depth': 6,\n",
    "#         'use_best_model': True,\n",
    "#         'subsample': 0.6,\n",
    "#         'bootstrap_type': 'Bernoulli',\n",
    "#         'reg_lambda': 3,\n",
    "#         'one_hot_max_size': 2,\n",
    "#     }\n",
    "#     model_cb = CatBoostRegressor(**cb_params)\n",
    "#     # train the model\n",
    "#     model_cb.fit(k_x_train, k_y_train, eval_set=[(k_x_vali, k_y_vali)], verbose=300, early_stopping_rounds=600)\n",
    "#     oof_cb[vali_index] = model_cb.predict(k_x_vali, ntree_end=model_cb.best_iteration_)\n",
    "#     predictions_cb += model_cb.predict(X_test, ntree_end=model_cb.best_iteration_) / kfolder.n_splits\n",
    "#     predictions_train_cb += model_cb.predict(X_data, ntree_end=model_cb.best_iteration_) / kfolder.n_splits\n",
    "#\n",
    "# print(\"catboost score: {:<8.8f}\".format(mean_absolute_error(np.expm1(oof_cb), np.expm1(Y_data))))\n",
    "#\n",
    "# output_path = path + '/user_data/'\n",
    "# # 测试集输出\n",
    "# predictions = predictions_cb\n",
    "# predictions[predictions < 0] = 0\n",
    "# sub = pd.DataFrame()\n",
    "# sub['SaleID'] = TestA_data.SaleID\n",
    "# sub['price'] = predictions\n",
    "# sub.to_csv(output_path+'cab_test.csv', index=False)\n",
    "#\n",
    "#\n",
    "# # 验证集输出\n",
    "# oof_cb[oof_cb < 0] = 0\n",
    "# sub = pd.DataFrame()\n",
    "# sub['SaleID'] = Train_data.SaleID\n",
    "# sub['price'] = oof_cb\n",
    "# sub.to_csv(output_path+'cab_train.csv', index=False)\n",
    "#\n",
    "# \"\"\"\n",
    "# 神经网络\n",
    "# \"\"\"\n",
    "# ## 读取神经网络模型数据\n",
    "# path = os.path.abspath(os.path.dirname(os.getcwd()) + os.path.sep + \".\")\n",
    "# tree_data_path = path+'/user_data/'\n",
    "# Train_NN_data = pd.read_csv(tree_data_path+'train_nn.csv', sep=' ')\n",
    "# Test_NN_data = pd.read_csv(tree_data_path+'test_nn.csv', sep=' ')\n",
    "#\n",
    "# numerical_cols = Train_NN_data.columns\n",
    "# feature_cols = [col for col in numerical_cols if col not in ['price','SaleID']]\n",
    "# ## 提前特征列，标签列构造训练样本和测试样本\n",
    "# X_data = Train_NN_data[feature_cols]\n",
    "# X_test = Test_NN_data[feature_cols]\n",
    "#\n",
    "#\n",
    "# x = np.array(X_data)\n",
    "# y = np.array(Train_NN_data['price'])\n",
    "# x_test = np.array(X_test)\n",
    "#\n",
    "#\n",
    "# #调整训练过程的学习率\n",
    "# def scheduler(epoch):\n",
    "#     # 到规定的epoch，学习率减小为原来的1/10\n",
    "#\n",
    "#     if epoch  == 1400 :\n",
    "#         lr = K.get_value(model.optimizer.lr)\n",
    "#         K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "#         print(\"lr changed to {}\".format(lr * 0.1))\n",
    "#     if epoch  == 1700 :\n",
    "#         lr = K.get_value(model.optimizer.lr)\n",
    "#         K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "#         print(\"lr changed to {}\".format(lr * 0.1))\n",
    "#     if epoch  == 1900 :\n",
    "#         lr = K.get_value(model.optimizer.lr)\n",
    "#         K.set_value(model.optimizer.lr, lr * 0.1)\n",
    "#         print(\"lr changed to {}\".format(lr * 0.1))\n",
    "#     return K.get_value(model.optimizer.lr)\n",
    "#\n",
    "# reduce_lr = LearningRateScheduler(scheduler)\n",
    "#\n",
    "#\n",
    "# kfolder = KFold(n_splits=10, shuffle=True, random_state=2018)\n",
    "# oof_nn = np.zeros(len(x))\n",
    "# predictions_nn = np.zeros(len(x_test))\n",
    "# predictions_train_nn = np.zeros(len(x))\n",
    "# kfold = kfolder.split(x, y)\n",
    "# fold_ = 0\n",
    "# for train_index, vali_index in kfold:\n",
    "#     k_x_train = x[train_index]\n",
    "#     k_y_train = y[train_index]\n",
    "#     k_x_vali = x[vali_index]\n",
    "#     k_y_vali = y[vali_index]\n",
    "#\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
    "#     model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
    "#     model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
    "#     model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
    "#     model.add(tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.02)))\n",
    "#\n",
    "#     model.compile(loss='mean_absolute_error',\n",
    "#                     optimizer=tf.keras.optimizers.Adam(),\n",
    "#                   metrics=['mae'])\n",
    "#\n",
    "#     model.fit(k_x_train,k_y_train,batch_size =512,epochs=2000,validation_data=(k_x_vali, k_y_vali), callbacks=[reduce_lr])#callbacks=callbacks,\n",
    "#     oof_nn[vali_index] = model.predict(k_x_vali).reshape((model.predict(k_x_vali).shape[0],))\n",
    "#     predictions_nn += model.predict(x_test).reshape((model.predict(x_test).shape[0],)) / kfolder.n_splits\n",
    "#     predictions_train_nn += model.predict(x).reshape((model.predict(x).shape[0],)) / kfolder.n_splits\n",
    "#\n",
    "# print(\"NN score: {:<8.8f}\".format(mean_absolute_error(oof_nn, y)))\n",
    "#\n",
    "# output_path = path + '/user_data/'\n",
    "# # 测试集输出\n",
    "# predictions = predictions_nn\n",
    "# predictions[predictions < 0] = 0\n",
    "# sub = pd.DataFrame()\n",
    "# sub['SaleID'] = Test_NN_data.SaleID\n",
    "# sub['price'] = predictions\n",
    "# sub.to_csv(output_path+'nn_test.csv', index=False)\n",
    "#\n",
    "# # 验证集输出\n",
    "# oof_nn[oof_nn < 0] = 0\n",
    "# sub = pd.DataFrame()\n",
    "# sub['SaleID'] = Train_NN_data.SaleID\n",
    "# sub['price'] = oof_nn\n",
    "# sub.to_csv(output_path+'nn_train.csv', index=False)\n",
    "#\n",
    "#\n",
    "# tree_data_path = path+'/user_data/'\n",
    "#\n",
    "# #导入树模型lgb预测数据，进行二层stacking输出\n",
    "# predictions_lgb = np.array(pd.read_csv(tree_data_path+'lgb_test.csv')['price'])\n",
    "# oof_lgb = np.array(pd.read_csv(tree_data_path+'lgb_train.csv')['price'])\n",
    "#\n",
    "# #导入树模型cab预测数据，进行二层stacking输出\n",
    "# predictions_cb = np.array(pd.read_csv(tree_data_path+'cab_test.csv')['price'])\n",
    "# oof_cb = np.array(pd.read_csv(tree_data_path+'cab_train.csv')['price'])\n",
    "#\n",
    "# #读取price，对验证集进行评估\n",
    "# Train_data = pd.read_csv(tree_data_path+'train_tree.csv', sep=' ')\n",
    "# TestA_data = pd.read_csv(tree_data_path+'text_tree.csv', sep=' ')\n",
    "# Y_data = Train_data['price']\n",
    "#\n",
    "# train_stack = np.vstack([oof_lgb, oof_cb]).transpose()\n",
    "# test_stack = np.vstack([predictions_lgb, predictions_cb]).transpose()\n",
    "# folds_stack = RepeatedKFold(n_splits=10, n_repeats=2, random_state=2018)\n",
    "# tree_stack = np.zeros(train_stack.shape[0])\n",
    "# predictions = np.zeros(test_stack.shape[0])\n",
    "#\n",
    "# #二层贝叶斯回归stack\n",
    "# for fold_, (trn_idx, val_idx) in enumerate(folds_stack.split(train_stack, Y_data)):\n",
    "#     print(\"fold {}\".format(fold_))\n",
    "#     trn_data, trn_y = train_stack[trn_idx], Y_data[trn_idx]\n",
    "#     val_data, val_y = train_stack[val_idx], Y_data[val_idx]\n",
    "#\n",
    "#     Bayes = linear_model.BayesianRidge()\n",
    "#     Bayes.fit(trn_data, trn_y)\n",
    "#     tree_stack[val_idx] = Bayes.predict(val_data)\n",
    "#     predictions += Bayes.predict(test_stack) / 20\n",
    "#\n",
    "# tree_predictions = np.expm1(predictions)\n",
    "# tree_stack = np.expm1(tree_stack)\n",
    "# tree_point = mean_absolute_error(tree_stack, np.expm1(Y_data))\n",
    "# print(\"树模型：二层贝叶斯: {:<8.8f}\".format(tree_point))\n",
    "#\n",
    "#\n",
    "#\n",
    "# #导入神经网络模型预测训练集数据，进行三层融合\n",
    "# predictions_nn = np.array(pd.read_csv(tree_data_path+'nn_test.csv')['price'])\n",
    "# oof_nn = np.array(pd.read_csv(tree_data_path+'nn_train.csv')['price'])\n",
    "#\n",
    "# nn_point = mean_absolute_error(oof_nn, np.expm1(Y_data))\n",
    "# print(\"神经网络: {:<8.8f}\".format(nn_point))\n",
    "#\n",
    "# oof = (oof_nn + tree_stack)/2\n",
    "# predictions = (tree_predictions + predictions_nn)/2\n",
    "# all_point = mean_absolute_error(oof, np.expm1(Y_data))\n",
    "# print(\"总输出：三层融合: {:<8.8f}\".format(all_point))\n",
    "#\n",
    "#\n",
    "# output_path = path + '/prediction_result/'\n",
    "# # 测试集输出\n",
    "# sub = pd.DataFrame()\n",
    "# sub['SaleID'] = TestA_data.SaleID\n",
    "# predictions[predictions < 0] = 0\n",
    "# sub['price']=predictions\n",
    "# sub.to_csv(output_path+'predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
